{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_TCN_RF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "w1sVV2CSbv3t"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3catz/deeplearning_timeseries/blob/master/EEG_TCN_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ax--B1rIbrD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Beginnings"
      ]
    },
    {
      "metadata": {
        "id": "rydY9rHUOoWs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "import os \n",
        "#os.mkdir(\"/content/subj1\")\n",
        "#os.chdir(\"/content/subj1\")\n",
        "os.listdir()\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-QZ7jrKybQy-",
        "colab_type": "code",
        "outputId": "d8e728c4-6048-45d5-e0e5-816ae8d679a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'adc.json', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "_BwtarmgYny0",
        "colab_type": "code",
        "outputId": "f5824c9e-bff2-426a-e3c5-59189ef2430b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3247
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://peijinbucket/eeg_train /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://peijinbucket/eeg_train/subj10_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series2_data.csv...\n",
            "/ [0 files][    0.0 B/  8.4 MiB]                                                \r/ [0 files][    0.0 B/ 51.5 MiB]                                                \r/ [0 files][    0.0 B/ 85.9 MiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series2_events.csv...\n",
            "/ [0 files][    0.0 B/ 92.8 MiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series3_data.csv...\n",
            "/ [0 files][    0.0 B/114.9 MiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series4_data.csv...\n",
            "/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series4_events.csv...\n",
            "/ [0 files][    0.0 B/  1.6 GiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series8_events.csv...\n",
            "\\ [188/188 files][  3.0 GiB/  3.0 GiB] 100% Done  96.7 MiB/s ETA 00:00:00       \n",
            "Operation completed over 188 objects/3.0 GiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1Fag5aoYvZi",
        "colab_type": "code",
        "outputId": "49da496a-89ab-4107-c12e-7661df26e456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://peijinbucket/test /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://peijinbucket/test/subj10_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj12_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj10_series9_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \r/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \r/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj1_series10_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj11_series10_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj1_series9_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj12_series10_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj11_series9_data.csv...\n",
            "/ [0/24 files][    0.0 B/446.3 MiB]   0% Done                                   \rCopying gs://peijinbucket/test/subj2_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj2_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj3_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj3_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj4_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj4_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj5_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj5_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj6_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj6_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj7_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj7_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj8_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj8_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj9_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj9_series9_data.csv...\n",
            "/ [24/24 files][446.3 MiB/446.3 MiB] 100% Done                                  \n",
            "Operation completed over 24 objects/446.3 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oKZ_mY6RUvft",
        "colab_type": "code",
        "outputId": "3c7ca768-d365-4876-b94c-3513560724ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2774
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn\n",
        "!pip install keras-tcn\n",
        "!pip install scikit-multilearn\n",
        "     \n",
        "\n",
        "!pip install networkx tensorflow\n",
        "!git clone https://github.com/thunlp/OpenNE.git\n",
        "os.chdir(\"/content/OpenNE/src\")\n",
        "!python setup.py install \n",
        "!pip install liac-arff"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.20.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Collecting keras-tcn\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/bc/dcbdc24d80229022333150f42ff88ddf4c6793568f711a0d6fc1e83b102e/keras_tcn-2.3.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-2.3.5\n",
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 3.6MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.0rc1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.3.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0rc0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (5.1.2)\n",
            "Cloning into 'OpenNE'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 537 (delta 0), reused 6 (delta 0), pack-reused 530\u001b[K\n",
            "Receiving objects: 100% (537/537), 14.93 MiB | 6.61 MiB/s, done.\n",
            "Resolving deltas: 100% (286/286), done.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating openne.egg-info\n",
            "writing openne.egg-info/PKG-INFO\n",
            "writing dependency_links to openne.egg-info/dependency_links.txt\n",
            "writing top-level names to openne.egg-info/top_level.txt\n",
            "writing manifest file 'openne.egg-info/SOURCES.txt'\n",
            "writing manifest file 'openne.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/openne\n",
            "copying openne/lle.py -> build/lib/openne\n",
            "copying openne/grarep.py -> build/lib/openne\n",
            "copying openne/lap.py -> build/lib/openne\n",
            "copying openne/node2vec.py -> build/lib/openne\n",
            "copying openne/hope.py -> build/lib/openne\n",
            "copying openne/classify.py -> build/lib/openne\n",
            "copying openne/sdne.py -> build/lib/openne\n",
            "copying openne/walker.py -> build/lib/openne\n",
            "copying openne/graph.py -> build/lib/openne\n",
            "copying openne/line.py -> build/lib/openne\n",
            "copying openne/gf.py -> build/lib/openne\n",
            "copying openne/tadw.py -> build/lib/openne\n",
            "copying openne/__main__.py -> build/lib/openne\n",
            "copying openne/__init__.py -> build/lib/openne\n",
            "creating build/lib/openne/gcn\n",
            "copying openne/gcn/inits.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/train.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/gcnAPI.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/layers.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/utils.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/models.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/__init__.py -> build/lib/openne/gcn\n",
            "copying openne/gcn/metrics.py -> build/lib/openne/gcn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/lle.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/grarep.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/lap.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/node2vec.py -> build/bdist.linux-x86_64/egg/openne\n",
            "creating build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/inits.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/train.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/gcnAPI.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/layers.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/utils.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/models.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/__init__.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/gcn/metrics.py -> build/bdist.linux-x86_64/egg/openne/gcn\n",
            "copying build/lib/openne/hope.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/classify.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/sdne.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/walker.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/graph.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/line.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/gf.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/tadw.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/__main__.py -> build/bdist.linux-x86_64/egg/openne\n",
            "copying build/lib/openne/__init__.py -> build/bdist.linux-x86_64/egg/openne\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/lle.py to lle.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/grarep.py to grarep.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/lap.py to lap.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/node2vec.py to node2vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/inits.py to inits.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/gcnAPI.py to gcnAPI.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/layers.py to layers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/models.py to models.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gcn/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/hope.py to hope.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/classify.py to classify.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/sdne.py to sdne.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/walker.py to walker.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/graph.py to graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/line.py to line.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/gf.py to gf.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/tadw.py to tadw.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/openne/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying openne.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying openne.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying openne.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying openne.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying openne.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/openne-0.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing openne-0.0.0-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/openne-0.0.0-py3.6.egg\n",
            "Extracting openne-0.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding openne 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/openne-0.0.0-py3.6.egg\n",
            "Processing dependencies for openne==0.0.0\n",
            "Finished processing dependencies for openne==0.0.0\n",
            "Collecting liac-arff\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: liac-arff\n",
            "Successfully installed liac-arff-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uGX1uPGRR06s",
        "colab_type": "code",
        "outputId": "ed5ea871-88c0-45d4-f9dd-c3d1d829578e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import sqrt\n",
        "from numpy import split, array\n",
        "\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error, roc_curve, auc\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import ConvLSTM2D, Dense, Concatenate, Activation, Permute, Lambda, multiply, GlobalMaxPooling1D, Flatten, Bidirectional, TimeDistributed, RepeatVector, LSTM, Dropout, GRU, Conv1D, Conv2D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import *\n",
        "from keras.constraints import max_norm\n",
        "from keras.optimizers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import seed\n",
        "\n",
        "from keras.models import Input, Model\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical \n",
        "from tcn import TCN\n",
        "from google.colab import files \n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(5888)\n",
        "\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.ext import Keras\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
        "#from skmultilearn.base.problem_transformation import LabelPowerset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-caa8655aa7a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNearMiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_transformation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelPowerSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabelPowerSet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "k_l7Ok24Oihz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#############function to read data###########\n",
        "\n",
        "def prepare_data_train(fname):\n",
        "    \"\"\" read and prepare training data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    # events file\n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    # read event file\n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "#############function to read data###########\n",
        "\n",
        "def prepare_data_train(fname):\n",
        "    \"\"\" read and prepare training data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    # events file\n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    # read event file\n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "\n",
        "def prepare_data_test(fname):\n",
        "    \"\"\" read and prepare test data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    return data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "def data_preprocess_train(X):\n",
        "    X_prep=scaler.fit_transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n",
        "\n",
        "def data_preprocess_test(X):\n",
        "    X_prep=scaler.transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Gzgi2OoOih4",
        "colab_type": "code",
        "outputId": "364c72c2-2c53-477b-b1d3-0965df797867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "ids_tot = []\n",
        "pred_tot = []\n",
        "\n",
        "y_raw= []\n",
        "raw = []\n",
        "for subject in [1]:\n",
        "  fnames =  glob('/content/eeg_train/subj%d_series*_data.csv' % (subject))\n",
        "  for fname in fnames:\n",
        "    data,labels=prepare_data_train(fname)\n",
        "    raw.append(data)\n",
        "    y_raw.append(labels)\n",
        "\n",
        "X = pd.concat(raw)\n",
        "y = pd.concat(y_raw)\n",
        "    #transform in numpy array\n",
        "    #transform train data in numpy array\n",
        "X_train = np.asarray(X.astype(np.float32))\n",
        "X_train = data_preprocess_train(X_train)\n",
        "y = np.asarray(y.astype(np.float32))\n",
        "\n",
        "\n",
        "test = []\n",
        "idx=[]\n",
        "for subject in [1]:\n",
        "  fnames =  glob('/content/test/subj%d_series*_data.csv' % (subject))\n",
        "  \n",
        "  for fname in fnames:\n",
        "    data=prepare_data_test(fname)\n",
        "    test.append(data)\n",
        "    idx.append(np.array(data['id']))\n",
        "X_test = pd.concat(test)\n",
        "ids = np.concatenate(idx)\n",
        "ids_tot.append(ids)\n",
        "X_test = X_test.drop(['id' ], axis=1)#remove id\n",
        "    #transform test data in numpy array\n",
        "\n",
        "X_test = data_preprocess_test(np.array(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytEAppCds8EK",
        "colab_type": "code",
        "outputId": "9c893950-41b2-44e3-a950-0258f255df7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape, y.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1422392, 32), (1422392, 6), (233081, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "fwFvlw88rwz0",
        "colab_type": "code",
        "outputId": "63a548b5-bd7f-497b-9e81-41ed9362eaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "lp = LabelPowerset()\n",
        "yt = lp.transform(y)\n",
        "pd.Series(yt).value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1243589\n",
              "1       39000\n",
              "8       27670\n",
              "6       27670\n",
              "5       25013\n",
              "3       22624\n",
              "7       11330\n",
              "2        9120\n",
              "9        7256\n",
              "10       6731\n",
              "4        2389\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "xdfj_kC6sBrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nmiss = NearMiss(sampling_strategy = {0:20000})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GBczTTisilK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xt_res, yt_res = nmiss.fit_resample(X_train, yt) #undersample and then do the train test split "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJpNrN80HXjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "sns.set_context(\"notebook\", font_scale=1.5,\n",
        "                rc={\"lines.linewidth\": 2.5})\n",
        "RS = 123\n",
        "def fashion_scatter(x, colors):\n",
        "    # choose a color palette with seaborn.\n",
        "    num_classes = len(np.unique(colors))\n",
        "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
        "\n",
        "    # create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "\n",
        "    # add the labels for each digit corresponding to the label\n",
        "    txts = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "\n",
        "        # Position of each label at median of data points.\n",
        "\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(i), fontsize=12)\n",
        "        txt.set_path_effects([\n",
        "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "            PathEffects.Normal()])\n",
        "        txts.append(txt)\n",
        "\n",
        "    return f, ax, sc, txts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NviLv5IbIK3S",
        "colab_type": "code",
        "outputId": "4bbb46b7-1990-4c22-8fd1-1110a5bab0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "pca_result = pca.fit_transform(X_train)\n",
        "\n",
        "print('PCA done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA done! Time elapsed: 4.282570123672485 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0eDOC2HOIiA4",
        "colab_type": "code",
        "outputId": "5ebdef7a-b793-48a7-a22d-2cdce2997054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pca_df = pd.DataFrame(columns = ['pca1','pca2','pca3','pca4'])\n",
        "\n",
        "pca_df['pca1'] = pca_result[:,0]\n",
        "pca_df['pca2'] = pca_result[:,1]\n",
        "pca_df['pca3'] = pca_result[:,2]\n",
        "pca_df['pca4'] = pca_result[:,3]\n",
        "\n",
        "print('Variance explained per principal component: {}'.format(pca.explained_variance_ratio_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance explained per principal component: [0.42718203 0.11557017 0.0709432  0.04517946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dLlO5SWtI0-m",
        "colab_type": "code",
        "outputId": "810c70ce-e51a-411f-9366-a6bf4b554565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "cell_type": "code",
      "source": [
        "top_two_comp = pca_df[['pca1','pca2']] # taking first and second principal component\n",
        "\n",
        "fashion_scatter(top_two_comp.values,yt) # Visualizing the PCA output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 576x576 with 1 Axes>,\n",
              " <matplotlib.axes._subplots.AxesSubplot at 0x7f029d1789b0>,\n",
              " <matplotlib.collections.PathCollection at 0x7f0298c3dd30>,\n",
              " [Text(0.2870195, -0.16683678, '0'),\n",
              "  Text(-1.0989608, -0.26527518, '1'),\n",
              "  Text(-1.8227584, -0.9480836, '2'),\n",
              "  Text(-1.2373312, -0.75737035, '3'),\n",
              "  Text(-0.8927579, -0.3360269, '4'),\n",
              "  Text(-1.0467259, -0.6193466, '5'),\n",
              "  Text(0.30665344, -1.4052598, '6'),\n",
              "  Text(0.6973564, -1.5040059, '7'),\n",
              "  Text(-0.13392898, -1.1091388, '8'),\n",
              "  Text(-1.1093204, -0.366948, '9'),\n",
              "  Text(-1.3401619, -0.5797329, '10')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAHWCAYAAACmHPpfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KxqFnQJcJnR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "time_start = time.time()\n",
        "my_tsne = TSNE(random_state=RS).fit_transform(X_train)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-C_94a9EJyRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_scatter(my_tsne, yt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQOP9mhlxY1F",
        "colab_type": "code",
        "outputId": "5eb36fa7-7c85-43a4-854a-62ae17c1b860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "pd.Series(yt_res).value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    39000\n",
              "9    20770\n",
              "7    20770\n",
              "6    19224\n",
              "8    18230\n",
              "3    18041\n",
              "4    11443\n",
              "2     9473\n",
              "5     8290\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "g67OJ2UILAj-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WillKoehrsen/feature-selector.git\n",
        "os.chdir(\"feature-selector\")\n",
        "!python setup.py install "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKX5yyGmLkAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from feature_selector import FeatureSelector\n",
        "# Features are in train and labels are in train_labels\n",
        "fs = FeatureSelector(data = pd.DataFrame(xt_res), labels = yt_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mb-JNNGuLwkl",
        "colab_type": "code",
        "outputId": "863e4671-b1a3-4d8f-c109-e2b4f6aa5e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "fs.identify_collinear(correlation_threshold = 0.75)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 features with a correlation magnitude greater than 0.75.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FJyVaU9JMhS-",
        "colab_type": "code",
        "outputId": "03ed95be-0228-4622-d481-c3a8d496d993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "collinear_features = fs.ops['collinear']\n",
        "print(collinear_features)\n",
        "fs.record_collinear.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[23, 24, 27, 28, 30, 31]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corr_feature</th>\n",
              "      <th>corr_value</th>\n",
              "      <th>drop_feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>0.792700</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23</td>\n",
              "      <td>0.752606</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>0.766958</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23</td>\n",
              "      <td>0.752740</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>0.788678</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  corr_feature  corr_value drop_feature\n",
              "0           22    0.792700           23\n",
              "1           23    0.752606           24\n",
              "2           22    0.766958           27\n",
              "3           23    0.752740           28\n",
              "4           29    0.788678           30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "2nEDnfBxWLmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xt_slim = pd.DataFrame(xt_res).drop(collinear_features, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npuL2n7yauM3",
        "colab_type": "code",
        "outputId": "9ad8f019-12ac-41dc-d341-d48894920133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(xt_slim, yt_res, test_size = 0.20, shuffle = True)\n",
        "print(x_train.shape, x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159042, 26) (39761, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hinkySm-XuD-",
        "colab_type": "code",
        "outputId": "0548eafb-afce-4f8d-c214-08b55e9c0169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69639, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "yFFfR1pXNSol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fs.identify_zero_importance(task = 'classification', \n",
        "                            eval_metric = 'auc', \n",
        "                            n_iterations = 10, \n",
        "                             early_stopping = True)\n",
        "# list of zero importance features\n",
        "zero_importance_features = fs.ops['zero_importance']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1sVV2CSbv3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest classifier\n"
      ]
    },
    {
      "metadata": {
        "id": "P7xPLf_1gncx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skmultilearn.embedding import SKLearnEmbedder, EmbeddingClassifier\n",
        "from sklearn.manifold import SpectralEmbedding\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from skmultilearn.adapt import MLkNN\n",
        "\n",
        "\n",
        "\n",
        "clf = BinaryRelevance(classifier = RandomForestClassifier(n_estimators=20, class_weight = \"balanced\", verbose=1, max_depth = 5), require_dense=[True,True])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiP1RwTmsIYP",
        "colab_type": "code",
        "outputId": "e8c5f75b-360b-4c0e-a77d-4f7ecec6367e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   34.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   32.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   34.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   33.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   33.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   36.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BinaryRelevance(classifier=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
              "            criterion='gini', max_depth=4, max_features='auto',\n",
              "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "            min_impurity_split=None, min_samples_leaf=1,\n",
              "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "            n_estimators=20, n_jobs=None, oob_score=False,\n",
              "            random_state=None, verbose=1, warm_start=False),\n",
              "        require_dense=[True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "6Hzf6lX88WW6",
        "colab_type": "code",
        "outputId": "46234474-014b-43c4-b440-fc6c5e610c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "result = clf.predict(x_val)\n",
        "result = result.toarray()\n",
        "\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "for index in range(6):\n",
        "  fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val[:,index], result[:,index])\n",
        "  auc_keras = auc(fpr_keras, tpr_keras)\n",
        "  print(auc_keras)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7035435265161376\n",
            "0.6631063674452841\n",
            "0.6504644578526994\n",
            "0.672597188947326\n",
            "0.6876487953075281\n",
            "0.6653013340099502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eo_o07IMv-Z-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Keras\n"
      ]
    },
    {
      "metadata": {
        "id": "hTpqI8oJmaxH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "weights = sklearn.utils.class_weight.compute_class_weight('balanced', range(12), yt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTJCxFpk-1Gc",
        "colab_type": "code",
        "outputId": "c405005a-7e5e-4b34-d125-017c87310921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.54670106e-02, 2.92558654e+00, 1.08272798e+01, 5.62876471e+00,\n",
              "       1.40601201e+01, 1.43736300e+01, 4.98886666e+00, 4.84811128e+00,\n",
              "       7.37757428e+00, 4.84811128e+00, 4.26295068e+01, 1.49147549e+03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "mD24mCJbt-cN",
        "colab_type": "code",
        "outputId": "9b5d7032-47fa-4528-cd92-2616694d8060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train = pd.DataFrame(y_train)\n",
        "set(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "mP4W4UPPtmrd",
        "colab_type": "code",
        "outputId": "bac28927-e8b2-4105-b7ae-2180e68293b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(132192,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "yy70XhactrmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozTprP8JzWNz",
        "colab_type": "code",
        "outputId": "2009e5b2-aa87-46bc-8092-73b840938635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "print(y_train.shape, y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159042, 11) (39761, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ClkDwT000GJh",
        "colab_type": "code",
        "outputId": "567d2a94-e363-4e0f-bba0-5a2a4c841e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(278553, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "sa252dCneAg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import keras \n",
        "from keras import backend\n",
        "from matplotlib import pyplot\n",
        "from math import pi\n",
        "from math import cos\n",
        "from math import floor\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(\"subj1.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=2)\n",
        "\n",
        "# define custom learning rate schedule\n",
        "class CosineAnnealingLearningRateSchedule(Callback):\n",
        "\t# constructor\n",
        "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
        "\t\tself.epochs = n_epochs\n",
        "\t\tself.cycles = n_cycles\n",
        "\t\tself.lr_max = lrate_max\n",
        "\t\tself.lrates = list()\n",
        "\n",
        "\t# calculate learning rate for an epoch\n",
        "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
        "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
        "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
        "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
        "\n",
        "\t# calculate and set learning rate at the start of the epoch\n",
        "\tdef on_epoch_begin(self, epoch, logs=None):\n",
        "\t\t# calculate learning rate\n",
        "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
        "\t\t# set learning rate\n",
        "\t\tbackend.set_value(self.model.optimizer.lr, lr)\n",
        "\t\t# log value\n",
        "\t\tself.lrates.append(lr)\n",
        "    \n",
        "ca = CosineAnnealingLearningRateSchedule(n_epochs = 500, n_cycles = 125, lrate_max = 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmUPu5W0irwx",
        "colab_type": "code",
        "outputId": "6ab52ba4-2cf4-4215-db6e-fd9275aa644c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25551
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Reshape\n",
        "def create_model_single_class(input_dim, output_dim):\n",
        "    # create model\n",
        "    i = Input(shape = (26,))\n",
        "    o = Reshape((1,26))(i)\n",
        "    o = TCN(64, nb_stacks = 1, dropout_rate = 0.5, use_skip_connections = True, return_sequences = False)(o)\n",
        "    o = Dense(11, activation = \"softmax\")(o)\n",
        "    model = Model(inputs=[i], outputs=[o])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer = adam(lr=1e-2), metrics=['acc'])\n",
        "    return model \n",
        "\n",
        "model = create_model_single_class(26,11)  \n",
        "\n",
        "model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = 500, batch_size = 2048, callbacks = [ca, checkpointer])\n",
        "#KERAS_PARAMS = dict(epochs=5, batch_size=10000,verbose=1, callbacks = [], class_weight = dict(enumerate(weights)))\n",
        "#clf = BinaryRelevance(classifier = Keras(create_model_single_class, False, KERAS_PARAMS), require_dense = [True, True]) \n",
        "#clf.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 159042 samples, validate on 39761 samples\n",
            "Epoch 1/500\n",
            "159042/159042 [==============================] - 6s 41us/step - loss: 1.8689 - acc: 0.3517 - val_loss: 1.6678 - val_acc: 0.4132\n",
            "Epoch 2/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.6568 - acc: 0.4168 - val_loss: 1.5142 - val_acc: 0.4598\n",
            "\n",
            "Epoch 00002: val_loss improved from inf to 1.51419, saving model to subj1.hdf5\n",
            "Epoch 3/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 1.5481 - acc: 0.4476 - val_loss: 1.4080 - val_acc: 0.4899\n",
            "Epoch 4/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.4750 - acc: 0.4693 - val_loss: 1.3498 - val_acc: 0.5095\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.51419 to 1.34981, saving model to subj1.hdf5\n",
            "Epoch 5/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.4854 - acc: 0.4663 - val_loss: 1.3165 - val_acc: 0.5189\n",
            "Epoch 6/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.4080 - acc: 0.4882 - val_loss: 1.2257 - val_acc: 0.5525\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.34981 to 1.22569, saving model to subj1.hdf5\n",
            "Epoch 7/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.3336 - acc: 0.5110 - val_loss: 1.1612 - val_acc: 0.5705\n",
            "Epoch 8/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.2797 - acc: 0.5293 - val_loss: 1.1137 - val_acc: 0.5870\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.22569 to 1.11372, saving model to subj1.hdf5\n",
            "Epoch 9/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.3258 - acc: 0.5134 - val_loss: 1.1413 - val_acc: 0.5756\n",
            "Epoch 10/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.2872 - acc: 0.5267 - val_loss: 1.0865 - val_acc: 0.5941\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.11372 to 1.08652, saving model to subj1.hdf5\n",
            "Epoch 11/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.2312 - acc: 0.5436 - val_loss: 1.0450 - val_acc: 0.6088\n",
            "Epoch 12/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1896 - acc: 0.5587 - val_loss: 1.0124 - val_acc: 0.6194\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.08652 to 1.01237, saving model to subj1.hdf5\n",
            "Epoch 13/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.2435 - acc: 0.5401 - val_loss: 1.0675 - val_acc: 0.6029\n",
            "Epoch 14/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.2237 - acc: 0.5460 - val_loss: 1.0217 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.01237\n",
            "Epoch 15/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.1750 - acc: 0.5627 - val_loss: 0.9764 - val_acc: 0.6327\n",
            "Epoch 16/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.1365 - acc: 0.5768 - val_loss: 0.9564 - val_acc: 0.6407\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.01237 to 0.95638, saving model to subj1.hdf5\n",
            "Epoch 17/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1891 - acc: 0.5583 - val_loss: 1.0083 - val_acc: 0.6242\n",
            "Epoch 18/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1764 - acc: 0.5622 - val_loss: 0.9747 - val_acc: 0.6316\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.95638\n",
            "Epoch 19/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1360 - acc: 0.5758 - val_loss: 0.9355 - val_acc: 0.6474\n",
            "Epoch 20/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1011 - acc: 0.5872 - val_loss: 0.9137 - val_acc: 0.6551\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.95638 to 0.91370, saving model to subj1.hdf5\n",
            "Epoch 21/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1512 - acc: 0.5708 - val_loss: 0.9641 - val_acc: 0.6379\n",
            "Epoch 22/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.1440 - acc: 0.5737 - val_loss: 0.9456 - val_acc: 0.6440\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.91370\n",
            "Epoch 23/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1056 - acc: 0.5855 - val_loss: 0.9142 - val_acc: 0.6538\n",
            "Epoch 24/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0761 - acc: 0.5954 - val_loss: 0.8875 - val_acc: 0.6660\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.91370 to 0.88751, saving model to subj1.hdf5\n",
            "Epoch 25/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1248 - acc: 0.5803 - val_loss: 0.9383 - val_acc: 0.6457\n",
            "Epoch 26/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.1218 - acc: 0.5814 - val_loss: 0.9138 - val_acc: 0.6572\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.88751\n",
            "Epoch 27/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0868 - acc: 0.5938 - val_loss: 0.8881 - val_acc: 0.6641\n",
            "Epoch 28/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0575 - acc: 0.6018 - val_loss: 0.8711 - val_acc: 0.6705\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.88751 to 0.87107, saving model to subj1.hdf5\n",
            "Epoch 29/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.1049 - acc: 0.5866 - val_loss: 0.9144 - val_acc: 0.6563\n",
            "Epoch 30/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.1006 - acc: 0.5875 - val_loss: 0.8932 - val_acc: 0.6620\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.87107\n",
            "Epoch 31/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0711 - acc: 0.5982 - val_loss: 0.8737 - val_acc: 0.6671\n",
            "Epoch 32/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0418 - acc: 0.6073 - val_loss: 0.8539 - val_acc: 0.6768\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.87107 to 0.85390, saving model to subj1.hdf5\n",
            "Epoch 33/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0897 - acc: 0.5916 - val_loss: 0.9104 - val_acc: 0.6543\n",
            "Epoch 34/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0848 - acc: 0.5925 - val_loss: 0.8800 - val_acc: 0.6673\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.85390\n",
            "Epoch 35/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0579 - acc: 0.6023 - val_loss: 0.8617 - val_acc: 0.6705\n",
            "Epoch 36/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0285 - acc: 0.6129 - val_loss: 0.8403 - val_acc: 0.6795\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.85390 to 0.84026, saving model to subj1.hdf5\n",
            "Epoch 37/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0756 - acc: 0.5958 - val_loss: 0.8867 - val_acc: 0.6618\n",
            "Epoch 38/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0698 - acc: 0.5982 - val_loss: 0.8737 - val_acc: 0.6697\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.84026\n",
            "Epoch 39/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0476 - acc: 0.6064 - val_loss: 0.8497 - val_acc: 0.6733\n",
            "Epoch 40/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0193 - acc: 0.6164 - val_loss: 0.8299 - val_acc: 0.6838\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.84026 to 0.82987, saving model to subj1.hdf5\n",
            "Epoch 41/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0659 - acc: 0.6004 - val_loss: 0.8742 - val_acc: 0.6652\n",
            "Epoch 42/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0605 - acc: 0.6010 - val_loss: 0.8558 - val_acc: 0.6727\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.82987\n",
            "Epoch 43/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0371 - acc: 0.6091 - val_loss: 0.8437 - val_acc: 0.6788\n",
            "Epoch 44/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0143 - acc: 0.6173 - val_loss: 0.8372 - val_acc: 0.6793\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.82987\n",
            "Epoch 45/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0583 - acc: 0.6027 - val_loss: 0.8602 - val_acc: 0.6690\n",
            "Epoch 46/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0482 - acc: 0.6062 - val_loss: 0.8527 - val_acc: 0.6756\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.82987\n",
            "Epoch 47/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0264 - acc: 0.6120 - val_loss: 0.8279 - val_acc: 0.6836\n",
            "Epoch 48/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9999 - acc: 0.6229 - val_loss: 0.8130 - val_acc: 0.6897\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.82987 to 0.81304, saving model to subj1.hdf5\n",
            "Epoch 49/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0455 - acc: 0.6064 - val_loss: 0.8557 - val_acc: 0.6724\n",
            "Epoch 50/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0435 - acc: 0.6063 - val_loss: 0.8368 - val_acc: 0.6798\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.81304\n",
            "Epoch 51/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0175 - acc: 0.6161 - val_loss: 0.8214 - val_acc: 0.6843\n",
            "Epoch 52/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9957 - acc: 0.6243 - val_loss: 0.8067 - val_acc: 0.6915\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.81304 to 0.80667, saving model to subj1.hdf5\n",
            "Epoch 53/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0365 - acc: 0.6102 - val_loss: 0.8527 - val_acc: 0.6742\n",
            "Epoch 54/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0389 - acc: 0.6086 - val_loss: 0.8318 - val_acc: 0.6823\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.80667\n",
            "Epoch 55/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0125 - acc: 0.6170 - val_loss: 0.8125 - val_acc: 0.6907\n",
            "Epoch 56/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9872 - acc: 0.6266 - val_loss: 0.7994 - val_acc: 0.6954\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.80667 to 0.79937, saving model to subj1.hdf5\n",
            "Epoch 57/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0287 - acc: 0.6120 - val_loss: 0.8322 - val_acc: 0.6814\n",
            "Epoch 58/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0280 - acc: 0.6132 - val_loss: 0.8295 - val_acc: 0.6853\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.79937\n",
            "Epoch 59/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0060 - acc: 0.6204 - val_loss: 0.8054 - val_acc: 0.6920\n",
            "Epoch 60/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9825 - acc: 0.6302 - val_loss: 0.7908 - val_acc: 0.6992\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.79937 to 0.79076, saving model to subj1.hdf5\n",
            "Epoch 61/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0266 - acc: 0.6128 - val_loss: 0.8310 - val_acc: 0.6820\n",
            "Epoch 62/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0228 - acc: 0.6141 - val_loss: 0.8116 - val_acc: 0.6902\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.79076\n",
            "Epoch 63/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9987 - acc: 0.6231 - val_loss: 0.8034 - val_acc: 0.6908\n",
            "Epoch 64/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9783 - acc: 0.6293 - val_loss: 0.7924 - val_acc: 0.6964\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.79076\n",
            "Epoch 65/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0174 - acc: 0.6172 - val_loss: 0.8256 - val_acc: 0.6826\n",
            "Epoch 66/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0176 - acc: 0.6162 - val_loss: 0.8199 - val_acc: 0.6876\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.79076\n",
            "Epoch 67/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9931 - acc: 0.6243 - val_loss: 0.7984 - val_acc: 0.6954\n",
            "Epoch 68/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9710 - acc: 0.6330 - val_loss: 0.7851 - val_acc: 0.6996\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.79076 to 0.78513, saving model to subj1.hdf5\n",
            "Epoch 69/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0114 - acc: 0.6190 - val_loss: 0.8250 - val_acc: 0.6842\n",
            "Epoch 70/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0108 - acc: 0.6195 - val_loss: 0.8140 - val_acc: 0.6923\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.78513\n",
            "Epoch 71/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9897 - acc: 0.6259 - val_loss: 0.7887 - val_acc: 0.6975\n",
            "Epoch 72/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9649 - acc: 0.6350 - val_loss: 0.7787 - val_acc: 0.7014\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.78513 to 0.77866, saving model to subj1.hdf5\n",
            "Epoch 73/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0061 - acc: 0.6218 - val_loss: 0.8182 - val_acc: 0.6917\n",
            "Epoch 74/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 1.0071 - acc: 0.6202 - val_loss: 0.8036 - val_acc: 0.6932\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.77866\n",
            "Epoch 75/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9857 - acc: 0.6278 - val_loss: 0.7892 - val_acc: 0.6963\n",
            "Epoch 76/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9622 - acc: 0.6343 - val_loss: 0.7756 - val_acc: 0.7035\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.77866 to 0.77562, saving model to subj1.hdf5\n",
            "Epoch 77/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0061 - acc: 0.6211 - val_loss: 0.8109 - val_acc: 0.6887\n",
            "Epoch 78/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0055 - acc: 0.6212 - val_loss: 0.7997 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.77562\n",
            "Epoch 79/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9807 - acc: 0.6295 - val_loss: 0.7882 - val_acc: 0.6988\n",
            "Epoch 80/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9613 - acc: 0.6370 - val_loss: 0.7745 - val_acc: 0.7046\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.77562 to 0.77447, saving model to subj1.hdf5\n",
            "Epoch 81/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 1.0006 - acc: 0.6216 - val_loss: 0.8113 - val_acc: 0.6950\n",
            "Epoch 82/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9998 - acc: 0.6231 - val_loss: 0.7966 - val_acc: 0.6940\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.77447\n",
            "Epoch 83/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9745 - acc: 0.6325 - val_loss: 0.7873 - val_acc: 0.7006\n",
            "Epoch 84/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9562 - acc: 0.6380 - val_loss: 0.7691 - val_acc: 0.7065\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.77447 to 0.76911, saving model to subj1.hdf5\n",
            "Epoch 85/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9953 - acc: 0.6244 - val_loss: 0.8099 - val_acc: 0.6874\n",
            "Epoch 86/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9987 - acc: 0.6236 - val_loss: 0.7917 - val_acc: 0.6973\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.76911\n",
            "Epoch 87/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9704 - acc: 0.6328 - val_loss: 0.7789 - val_acc: 0.7011\n",
            "Epoch 88/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9510 - acc: 0.6409 - val_loss: 0.7680 - val_acc: 0.7060\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.76911 to 0.76805, saving model to subj1.hdf5\n",
            "Epoch 89/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9924 - acc: 0.6255 - val_loss: 0.8021 - val_acc: 0.6921\n",
            "Epoch 90/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9896 - acc: 0.6258 - val_loss: 0.7898 - val_acc: 0.6976\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.76805\n",
            "Epoch 91/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9726 - acc: 0.6313 - val_loss: 0.7719 - val_acc: 0.7021\n",
            "Epoch 92/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9499 - acc: 0.6408 - val_loss: 0.7647 - val_acc: 0.7073\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.76805 to 0.76467, saving model to subj1.hdf5\n",
            "Epoch 93/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9884 - acc: 0.6268 - val_loss: 0.7954 - val_acc: 0.6981\n",
            "Epoch 94/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9925 - acc: 0.6252 - val_loss: 0.7889 - val_acc: 0.7005\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.76467\n",
            "Epoch 95/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9671 - acc: 0.6337 - val_loss: 0.7736 - val_acc: 0.7026\n",
            "Epoch 96/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9493 - acc: 0.6402 - val_loss: 0.7588 - val_acc: 0.7095\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.76467 to 0.75881, saving model to subj1.hdf5\n",
            "Epoch 97/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9841 - acc: 0.6280 - val_loss: 0.7989 - val_acc: 0.6937\n",
            "Epoch 98/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9833 - acc: 0.6284 - val_loss: 0.7858 - val_acc: 0.6981\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.75881\n",
            "Epoch 99/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9609 - acc: 0.6363 - val_loss: 0.7703 - val_acc: 0.7056\n",
            "Epoch 100/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9431 - acc: 0.6433 - val_loss: 0.7575 - val_acc: 0.7106\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.75881 to 0.75748, saving model to subj1.hdf5\n",
            "Epoch 101/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9817 - acc: 0.6300 - val_loss: 0.7890 - val_acc: 0.6981\n",
            "Epoch 102/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9805 - acc: 0.6295 - val_loss: 0.7825 - val_acc: 0.7016\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.75748\n",
            "Epoch 103/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9574 - acc: 0.6369 - val_loss: 0.7651 - val_acc: 0.7087\n",
            "Epoch 104/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9417 - acc: 0.6428 - val_loss: 0.7528 - val_acc: 0.7137\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.75748 to 0.75284, saving model to subj1.hdf5\n",
            "Epoch 105/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9808 - acc: 0.6289 - val_loss: 0.7882 - val_acc: 0.7004\n",
            "Epoch 106/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9838 - acc: 0.6278 - val_loss: 0.7869 - val_acc: 0.6985\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.75284\n",
            "Epoch 107/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9605 - acc: 0.6370 - val_loss: 0.7708 - val_acc: 0.7052\n",
            "Epoch 108/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9400 - acc: 0.6429 - val_loss: 0.7557 - val_acc: 0.7111\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.75284\n",
            "Epoch 109/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9780 - acc: 0.6311 - val_loss: 0.7906 - val_acc: 0.6961\n",
            "Epoch 110/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9774 - acc: 0.6314 - val_loss: 0.7809 - val_acc: 0.7012\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.75284\n",
            "Epoch 111/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9574 - acc: 0.6378 - val_loss: 0.7688 - val_acc: 0.7052\n",
            "Epoch 112/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9357 - acc: 0.6448 - val_loss: 0.7521 - val_acc: 0.7132\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.75284 to 0.75210, saving model to subj1.hdf5\n",
            "Epoch 113/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9787 - acc: 0.6301 - val_loss: 0.7814 - val_acc: 0.7027\n",
            "Epoch 114/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9739 - acc: 0.6310 - val_loss: 0.7738 - val_acc: 0.7015\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.75210\n",
            "Epoch 115/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9531 - acc: 0.6375 - val_loss: 0.7665 - val_acc: 0.7061\n",
            "Epoch 116/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9352 - acc: 0.6464 - val_loss: 0.7507 - val_acc: 0.7139\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.75210 to 0.75067, saving model to subj1.hdf5\n",
            "Epoch 117/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9693 - acc: 0.6340 - val_loss: 0.7772 - val_acc: 0.7017\n",
            "Epoch 118/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9745 - acc: 0.6304 - val_loss: 0.7795 - val_acc: 0.7028\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.75067\n",
            "Epoch 119/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9707 - acc: 0.6329 - val_loss: 0.7988 - val_acc: 0.6957\n",
            "Epoch 120/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9622 - acc: 0.6361 - val_loss: 0.7628 - val_acc: 0.7089\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.75067\n",
            "Epoch 121/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9734 - acc: 0.6314 - val_loss: 0.7775 - val_acc: 0.7035\n",
            "Epoch 122/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9695 - acc: 0.6344 - val_loss: 0.7741 - val_acc: 0.7032\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.75067\n",
            "Epoch 123/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9486 - acc: 0.6402 - val_loss: 0.7600 - val_acc: 0.7086\n",
            "Epoch 124/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9313 - acc: 0.6463 - val_loss: 0.7496 - val_acc: 0.7147\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.75067 to 0.74961, saving model to subj1.hdf5\n",
            "Epoch 125/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9654 - acc: 0.6355 - val_loss: 0.7710 - val_acc: 0.7063\n",
            "Epoch 126/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9668 - acc: 0.6337 - val_loss: 0.7741 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.74961\n",
            "Epoch 127/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9497 - acc: 0.6396 - val_loss: 0.7553 - val_acc: 0.7126\n",
            "Epoch 128/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9292 - acc: 0.6473 - val_loss: 0.7463 - val_acc: 0.7170\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.74961 to 0.74630, saving model to subj1.hdf5\n",
            "Epoch 129/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9656 - acc: 0.6344 - val_loss: 0.7730 - val_acc: 0.7054\n",
            "Epoch 130/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9628 - acc: 0.6363 - val_loss: 0.7596 - val_acc: 0.7105\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.74630\n",
            "Epoch 131/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9469 - acc: 0.6412 - val_loss: 0.7560 - val_acc: 0.7133\n",
            "Epoch 132/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9274 - acc: 0.6498 - val_loss: 0.7443 - val_acc: 0.7172\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.74630 to 0.74426, saving model to subj1.hdf5\n",
            "Epoch 133/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9622 - acc: 0.6366 - val_loss: 0.7781 - val_acc: 0.7016\n",
            "Epoch 134/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9650 - acc: 0.6343 - val_loss: 0.7698 - val_acc: 0.7061\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.74426\n",
            "Epoch 135/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9434 - acc: 0.6415 - val_loss: 0.7477 - val_acc: 0.7145\n",
            "Epoch 136/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9378 - acc: 0.6449 - val_loss: 0.7481 - val_acc: 0.7164\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.74426\n",
            "Epoch 137/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9575 - acc: 0.6372 - val_loss: 0.7637 - val_acc: 0.7084\n",
            "Epoch 138/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9585 - acc: 0.6374 - val_loss: 0.7677 - val_acc: 0.7057\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.74426\n",
            "Epoch 139/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9421 - acc: 0.6425 - val_loss: 0.7496 - val_acc: 0.7152\n",
            "Epoch 140/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9264 - acc: 0.6499 - val_loss: 0.7394 - val_acc: 0.7197\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.74426 to 0.73942, saving model to subj1.hdf5\n",
            "Epoch 141/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9573 - acc: 0.6377 - val_loss: 0.7722 - val_acc: 0.7077\n",
            "Epoch 142/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9572 - acc: 0.6371 - val_loss: 0.7686 - val_acc: 0.7100\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.73942\n",
            "Epoch 143/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9443 - acc: 0.6434 - val_loss: 0.7513 - val_acc: 0.7135\n",
            "Epoch 144/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9234 - acc: 0.6504 - val_loss: 0.7377 - val_acc: 0.7217\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.73942 to 0.73775, saving model to subj1.hdf5\n",
            "Epoch 145/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9539 - acc: 0.6400 - val_loss: 0.7767 - val_acc: 0.7040\n",
            "Epoch 146/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9538 - acc: 0.6386 - val_loss: 0.7637 - val_acc: 0.7101\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.73775\n",
            "Epoch 147/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9405 - acc: 0.6430 - val_loss: 0.7465 - val_acc: 0.7151\n",
            "Epoch 148/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9208 - acc: 0.6511 - val_loss: 0.7356 - val_acc: 0.7213\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.73775 to 0.73560, saving model to subj1.hdf5\n",
            "Epoch 149/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9522 - acc: 0.6397 - val_loss: 0.7630 - val_acc: 0.7095\n",
            "Epoch 150/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9554 - acc: 0.6385 - val_loss: 0.7592 - val_acc: 0.7107\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.73560\n",
            "Epoch 151/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9388 - acc: 0.6439 - val_loss: 0.7434 - val_acc: 0.7158\n",
            "Epoch 152/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9206 - acc: 0.6514 - val_loss: 0.7344 - val_acc: 0.7209\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.73560 to 0.73443, saving model to subj1.hdf5\n",
            "Epoch 153/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9530 - acc: 0.6404 - val_loss: 0.7675 - val_acc: 0.7090\n",
            "Epoch 154/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9551 - acc: 0.6393 - val_loss: 0.7596 - val_acc: 0.7070\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.73443\n",
            "Epoch 155/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9346 - acc: 0.6453 - val_loss: 0.7421 - val_acc: 0.7163\n",
            "Epoch 156/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9187 - acc: 0.6507 - val_loss: 0.7362 - val_acc: 0.7187\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.73443\n",
            "Epoch 157/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9518 - acc: 0.6392 - val_loss: 0.7636 - val_acc: 0.7079\n",
            "Epoch 158/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9493 - acc: 0.6412 - val_loss: 0.7697 - val_acc: 0.7044\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.73443\n",
            "Epoch 159/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9340 - acc: 0.6459 - val_loss: 0.7427 - val_acc: 0.7157\n",
            "Epoch 160/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9173 - acc: 0.6518 - val_loss: 0.7318 - val_acc: 0.7202\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.73443 to 0.73181, saving model to subj1.hdf5\n",
            "Epoch 161/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9511 - acc: 0.6394 - val_loss: 0.7627 - val_acc: 0.7101\n",
            "Epoch 162/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9544 - acc: 0.6387 - val_loss: 0.7571 - val_acc: 0.7097\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.73181\n",
            "Epoch 163/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9323 - acc: 0.6479 - val_loss: 0.7401 - val_acc: 0.7173\n",
            "Epoch 164/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9144 - acc: 0.6535 - val_loss: 0.7310 - val_acc: 0.7217\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.73181 to 0.73102, saving model to subj1.hdf5\n",
            "Epoch 165/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9497 - acc: 0.6393 - val_loss: 0.7629 - val_acc: 0.7076\n",
            "Epoch 166/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9529 - acc: 0.6390 - val_loss: 0.7604 - val_acc: 0.7102\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.73102\n",
            "Epoch 167/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9287 - acc: 0.6480 - val_loss: 0.7439 - val_acc: 0.7154\n",
            "Epoch 168/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9147 - acc: 0.6546 - val_loss: 0.7302 - val_acc: 0.7222\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.73102 to 0.73020, saving model to subj1.hdf5\n",
            "Epoch 169/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9464 - acc: 0.6424 - val_loss: 0.7532 - val_acc: 0.7147\n",
            "Epoch 170/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9469 - acc: 0.6425 - val_loss: 0.7505 - val_acc: 0.7140\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.73020\n",
            "Epoch 171/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9338 - acc: 0.6464 - val_loss: 0.7407 - val_acc: 0.7175\n",
            "Epoch 172/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9128 - acc: 0.6544 - val_loss: 0.7317 - val_acc: 0.7209\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.73020\n",
            "Epoch 173/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9444 - acc: 0.6422 - val_loss: 0.7523 - val_acc: 0.7125\n",
            "Epoch 174/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9465 - acc: 0.6412 - val_loss: 0.7597 - val_acc: 0.7090\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.73020\n",
            "Epoch 175/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9316 - acc: 0.6459 - val_loss: 0.7364 - val_acc: 0.7198\n",
            "Epoch 176/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9113 - acc: 0.6540 - val_loss: 0.7275 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.73020 to 0.72749, saving model to subj1.hdf5\n",
            "Epoch 177/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9433 - acc: 0.6433 - val_loss: 0.7664 - val_acc: 0.7087\n",
            "Epoch 178/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9411 - acc: 0.6433 - val_loss: 0.7498 - val_acc: 0.7160\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.72749\n",
            "Epoch 179/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9270 - acc: 0.6489 - val_loss: 0.7388 - val_acc: 0.7160\n",
            "Epoch 180/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9106 - acc: 0.6533 - val_loss: 0.7286 - val_acc: 0.7234\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.72749\n",
            "Epoch 181/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9441 - acc: 0.6424 - val_loss: 0.7493 - val_acc: 0.7145\n",
            "Epoch 182/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9426 - acc: 0.6429 - val_loss: 0.7506 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.72749\n",
            "Epoch 183/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9296 - acc: 0.6469 - val_loss: 0.7353 - val_acc: 0.7196\n",
            "Epoch 184/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9122 - acc: 0.6539 - val_loss: 0.7251 - val_acc: 0.7238\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.72749 to 0.72514, saving model to subj1.hdf5\n",
            "Epoch 185/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9427 - acc: 0.6434 - val_loss: 0.7561 - val_acc: 0.7121\n",
            "Epoch 186/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9559 - acc: 0.6383 - val_loss: 0.7516 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.72514\n",
            "Epoch 187/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9303 - acc: 0.6474 - val_loss: 0.7379 - val_acc: 0.7179\n",
            "Epoch 188/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9103 - acc: 0.6535 - val_loss: 0.7256 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.72514\n",
            "Epoch 189/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9409 - acc: 0.6435 - val_loss: 0.7577 - val_acc: 0.7092\n",
            "Epoch 190/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9425 - acc: 0.6437 - val_loss: 0.7492 - val_acc: 0.7158\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.72514\n",
            "Epoch 191/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9254 - acc: 0.6485 - val_loss: 0.7347 - val_acc: 0.7181\n",
            "Epoch 192/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9091 - acc: 0.6551 - val_loss: 0.7264 - val_acc: 0.7239\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.72514\n",
            "Epoch 193/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9442 - acc: 0.6440 - val_loss: 0.7511 - val_acc: 0.7131\n",
            "Epoch 194/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9403 - acc: 0.6439 - val_loss: 0.7459 - val_acc: 0.7196\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.72514\n",
            "Epoch 195/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9250 - acc: 0.6494 - val_loss: 0.7321 - val_acc: 0.7207\n",
            "Epoch 196/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9093 - acc: 0.6555 - val_loss: 0.7252 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.72514\n",
            "Epoch 197/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9366 - acc: 0.6461 - val_loss: 0.7525 - val_acc: 0.7113\n",
            "Epoch 198/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9376 - acc: 0.6454 - val_loss: 0.7476 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.72514\n",
            "Epoch 199/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9263 - acc: 0.6490 - val_loss: 0.7334 - val_acc: 0.7200\n",
            "Epoch 200/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9075 - acc: 0.6555 - val_loss: 0.7223 - val_acc: 0.7239\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.72514 to 0.72232, saving model to subj1.hdf5\n",
            "Epoch 201/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9398 - acc: 0.6453 - val_loss: 0.7438 - val_acc: 0.7158\n",
            "Epoch 202/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9410 - acc: 0.6429 - val_loss: 0.7407 - val_acc: 0.7179\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.72232\n",
            "Epoch 203/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9218 - acc: 0.6499 - val_loss: 0.7298 - val_acc: 0.7226\n",
            "Epoch 204/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9032 - acc: 0.6570 - val_loss: 0.7201 - val_acc: 0.7250\n",
            "\n",
            "Epoch 00204: val_loss improved from 0.72232 to 0.72005, saving model to subj1.hdf5\n",
            "Epoch 205/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9330 - acc: 0.6461 - val_loss: 0.7475 - val_acc: 0.7137\n",
            "Epoch 206/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9386 - acc: 0.6443 - val_loss: 0.7461 - val_acc: 0.7157\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.72005\n",
            "Epoch 207/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9232 - acc: 0.6496 - val_loss: 0.7268 - val_acc: 0.7244\n",
            "Epoch 208/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9085 - acc: 0.6552 - val_loss: 0.7200 - val_acc: 0.7256\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.72005 to 0.71997, saving model to subj1.hdf5\n",
            "Epoch 209/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9334 - acc: 0.6462 - val_loss: 0.7420 - val_acc: 0.7161\n",
            "Epoch 210/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9371 - acc: 0.6455 - val_loss: 0.7398 - val_acc: 0.7206\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.71997\n",
            "Epoch 211/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9209 - acc: 0.6514 - val_loss: 0.7317 - val_acc: 0.7203\n",
            "Epoch 212/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9001 - acc: 0.6576 - val_loss: 0.7199 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00212: val_loss improved from 0.71997 to 0.71991, saving model to subj1.hdf5\n",
            "Epoch 213/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9319 - acc: 0.6470 - val_loss: 0.7459 - val_acc: 0.7171\n",
            "Epoch 214/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9336 - acc: 0.6469 - val_loss: 0.7375 - val_acc: 0.7176\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.71991\n",
            "Epoch 215/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9183 - acc: 0.6515 - val_loss: 0.7266 - val_acc: 0.7240\n",
            "Epoch 216/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9054 - acc: 0.6571 - val_loss: 0.7198 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00216: val_loss improved from 0.71991 to 0.71975, saving model to subj1.hdf5\n",
            "Epoch 217/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9330 - acc: 0.6486 - val_loss: 0.7456 - val_acc: 0.7152\n",
            "Epoch 218/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9355 - acc: 0.6475 - val_loss: 0.7420 - val_acc: 0.7184\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.71975\n",
            "Epoch 219/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9181 - acc: 0.6514 - val_loss: 0.7247 - val_acc: 0.7237\n",
            "Epoch 220/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9022 - acc: 0.6565 - val_loss: 0.7201 - val_acc: 0.7260\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.71975\n",
            "Epoch 221/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9344 - acc: 0.6455 - val_loss: 0.7453 - val_acc: 0.7159\n",
            "Epoch 222/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9318 - acc: 0.6483 - val_loss: 0.7397 - val_acc: 0.7167\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.71975\n",
            "Epoch 223/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9168 - acc: 0.6527 - val_loss: 0.7293 - val_acc: 0.7205\n",
            "Epoch 224/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9038 - acc: 0.6577 - val_loss: 0.7193 - val_acc: 0.7261\n",
            "\n",
            "Epoch 00224: val_loss improved from 0.71975 to 0.71930, saving model to subj1.hdf5\n",
            "Epoch 225/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9329 - acc: 0.6489 - val_loss: 0.7406 - val_acc: 0.7153\n",
            "Epoch 226/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9324 - acc: 0.6470 - val_loss: 0.7408 - val_acc: 0.7176\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.71930\n",
            "Epoch 227/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9152 - acc: 0.6519 - val_loss: 0.7230 - val_acc: 0.7212\n",
            "Epoch 228/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9019 - acc: 0.6584 - val_loss: 0.7179 - val_acc: 0.7257\n",
            "\n",
            "Epoch 00228: val_loss improved from 0.71930 to 0.71786, saving model to subj1.hdf5\n",
            "Epoch 229/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9278 - acc: 0.6502 - val_loss: 0.7402 - val_acc: 0.7167\n",
            "Epoch 230/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9317 - acc: 0.6462 - val_loss: 0.7388 - val_acc: 0.7176\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.71786\n",
            "Epoch 231/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9137 - acc: 0.6549 - val_loss: 0.7247 - val_acc: 0.7229\n",
            "Epoch 232/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8993 - acc: 0.6598 - val_loss: 0.7166 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00232: val_loss improved from 0.71786 to 0.71663, saving model to subj1.hdf5\n",
            "Epoch 233/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9299 - acc: 0.6483 - val_loss: 0.7379 - val_acc: 0.7175\n",
            "Epoch 234/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9278 - acc: 0.6477 - val_loss: 0.7371 - val_acc: 0.7184\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.71663\n",
            "Epoch 235/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9108 - acc: 0.6542 - val_loss: 0.7211 - val_acc: 0.7239\n",
            "Epoch 236/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8946 - acc: 0.6600 - val_loss: 0.7138 - val_acc: 0.7253\n",
            "\n",
            "Epoch 00236: val_loss improved from 0.71663 to 0.71377, saving model to subj1.hdf5\n",
            "Epoch 237/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9256 - acc: 0.6500 - val_loss: 0.7370 - val_acc: 0.7169\n",
            "Epoch 238/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9268 - acc: 0.6495 - val_loss: 0.7379 - val_acc: 0.7157\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.71377\n",
            "Epoch 239/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9146 - acc: 0.6536 - val_loss: 0.7205 - val_acc: 0.7239\n",
            "Epoch 240/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8964 - acc: 0.6592 - val_loss: 0.7157 - val_acc: 0.7252\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.71377\n",
            "Epoch 241/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9280 - acc: 0.6480 - val_loss: 0.7411 - val_acc: 0.7136\n",
            "Epoch 242/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9283 - acc: 0.6479 - val_loss: 0.7343 - val_acc: 0.7184\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.71377\n",
            "Epoch 243/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9142 - acc: 0.6531 - val_loss: 0.7221 - val_acc: 0.7257\n",
            "Epoch 244/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8956 - acc: 0.6608 - val_loss: 0.7144 - val_acc: 0.7271\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.71377\n",
            "Epoch 245/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9294 - acc: 0.6489 - val_loss: 0.7405 - val_acc: 0.7180\n",
            "Epoch 246/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9268 - acc: 0.6492 - val_loss: 0.7311 - val_acc: 0.7205\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.71377\n",
            "Epoch 247/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9086 - acc: 0.6563 - val_loss: 0.7199 - val_acc: 0.7238\n",
            "Epoch 248/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8972 - acc: 0.6594 - val_loss: 0.7129 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00248: val_loss improved from 0.71377 to 0.71291, saving model to subj1.hdf5\n",
            "Epoch 249/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9225 - acc: 0.6498 - val_loss: 0.7377 - val_acc: 0.7186\n",
            "Epoch 250/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9275 - acc: 0.6495 - val_loss: 0.7352 - val_acc: 0.7162\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.71291\n",
            "Epoch 251/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9078 - acc: 0.6558 - val_loss: 0.7204 - val_acc: 0.7261\n",
            "Epoch 252/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8978 - acc: 0.6594 - val_loss: 0.7116 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00252: val_loss improved from 0.71291 to 0.71165, saving model to subj1.hdf5\n",
            "Epoch 253/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9237 - acc: 0.6494 - val_loss: 0.7372 - val_acc: 0.7190\n",
            "Epoch 254/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9257 - acc: 0.6500 - val_loss: 0.7336 - val_acc: 0.7193\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.71165\n",
            "Epoch 255/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9128 - acc: 0.6542 - val_loss: 0.7234 - val_acc: 0.7218\n",
            "Epoch 256/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8959 - acc: 0.6610 - val_loss: 0.7126 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.71165\n",
            "Epoch 257/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9240 - acc: 0.6503 - val_loss: 0.7329 - val_acc: 0.7185\n",
            "Epoch 258/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9233 - acc: 0.6495 - val_loss: 0.7367 - val_acc: 0.7194\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.71165\n",
            "Epoch 259/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9095 - acc: 0.6545 - val_loss: 0.7203 - val_acc: 0.7257\n",
            "Epoch 260/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8959 - acc: 0.6600 - val_loss: 0.7118 - val_acc: 0.7284\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.71165\n",
            "Epoch 261/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9241 - acc: 0.6494 - val_loss: 0.7360 - val_acc: 0.7158\n",
            "Epoch 262/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9227 - acc: 0.6502 - val_loss: 0.7337 - val_acc: 0.7189\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.71165\n",
            "Epoch 263/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9078 - acc: 0.6554 - val_loss: 0.7166 - val_acc: 0.7283\n",
            "Epoch 264/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8951 - acc: 0.6600 - val_loss: 0.7100 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00264: val_loss improved from 0.71165 to 0.71005, saving model to subj1.hdf5\n",
            "Epoch 265/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9225 - acc: 0.6487 - val_loss: 0.7326 - val_acc: 0.7218\n",
            "Epoch 266/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9199 - acc: 0.6510 - val_loss: 0.7319 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.71005\n",
            "Epoch 267/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9106 - acc: 0.6537 - val_loss: 0.7174 - val_acc: 0.7261\n",
            "Epoch 268/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8915 - acc: 0.6616 - val_loss: 0.7086 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00268: val_loss improved from 0.71005 to 0.70859, saving model to subj1.hdf5\n",
            "Epoch 269/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9178 - acc: 0.6499 - val_loss: 0.7399 - val_acc: 0.7157\n",
            "Epoch 270/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9226 - acc: 0.6495 - val_loss: 0.7319 - val_acc: 0.7225\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.70859\n",
            "Epoch 271/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9049 - acc: 0.6561 - val_loss: 0.7177 - val_acc: 0.7270\n",
            "Epoch 272/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8912 - acc: 0.6610 - val_loss: 0.7057 - val_acc: 0.7327\n",
            "\n",
            "Epoch 00272: val_loss improved from 0.70859 to 0.70568, saving model to subj1.hdf5\n",
            "Epoch 273/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9195 - acc: 0.6514 - val_loss: 0.7287 - val_acc: 0.7236\n",
            "Epoch 274/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9209 - acc: 0.6513 - val_loss: 0.7268 - val_acc: 0.7230\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.70568\n",
            "Epoch 275/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9079 - acc: 0.6560 - val_loss: 0.7168 - val_acc: 0.7265\n",
            "Epoch 276/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8895 - acc: 0.6643 - val_loss: 0.7082 - val_acc: 0.7304\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.70568\n",
            "Epoch 277/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9166 - acc: 0.6533 - val_loss: 0.7274 - val_acc: 0.7255\n",
            "Epoch 278/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9190 - acc: 0.6508 - val_loss: 0.7246 - val_acc: 0.7246\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.70568\n",
            "Epoch 279/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9066 - acc: 0.6562 - val_loss: 0.7174 - val_acc: 0.7257\n",
            "Epoch 280/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8929 - acc: 0.6617 - val_loss: 0.7067 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.70568\n",
            "Epoch 281/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9169 - acc: 0.6524 - val_loss: 0.7341 - val_acc: 0.7177\n",
            "Epoch 282/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9318 - acc: 0.6501 - val_loss: 0.7813 - val_acc: 0.7013\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.70568\n",
            "Epoch 283/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9399 - acc: 0.6450 - val_loss: 0.7293 - val_acc: 0.7229\n",
            "Epoch 284/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9057 - acc: 0.6574 - val_loss: 0.7203 - val_acc: 0.7241\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.70568\n",
            "Epoch 285/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9245 - acc: 0.6501 - val_loss: 0.7302 - val_acc: 0.7227\n",
            "Epoch 286/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9202 - acc: 0.6507 - val_loss: 0.7255 - val_acc: 0.7243\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.70568\n",
            "Epoch 287/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9078 - acc: 0.6560 - val_loss: 0.7164 - val_acc: 0.7250\n",
            "Epoch 288/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8944 - acc: 0.6606 - val_loss: 0.7074 - val_acc: 0.7301\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.70568\n",
            "Epoch 289/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9165 - acc: 0.6537 - val_loss: 0.7305 - val_acc: 0.7217\n",
            "Epoch 290/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9172 - acc: 0.6511 - val_loss: 0.7243 - val_acc: 0.7234\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.70568\n",
            "Epoch 291/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9035 - acc: 0.6583 - val_loss: 0.7131 - val_acc: 0.7276\n",
            "Epoch 292/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8905 - acc: 0.6626 - val_loss: 0.7075 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.70568\n",
            "Epoch 293/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9183 - acc: 0.6526 - val_loss: 0.7306 - val_acc: 0.7190\n",
            "Epoch 294/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9186 - acc: 0.6516 - val_loss: 0.7232 - val_acc: 0.7223\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.70568\n",
            "Epoch 295/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9041 - acc: 0.6572 - val_loss: 0.7138 - val_acc: 0.7273\n",
            "Epoch 296/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8889 - acc: 0.6636 - val_loss: 0.7064 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.70568\n",
            "Epoch 297/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9129 - acc: 0.6540 - val_loss: 0.7256 - val_acc: 0.7226\n",
            "Epoch 298/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9167 - acc: 0.6531 - val_loss: 0.7251 - val_acc: 0.7232\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.70568\n",
            "Epoch 299/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9012 - acc: 0.6584 - val_loss: 0.7141 - val_acc: 0.7257\n",
            "Epoch 300/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8856 - acc: 0.6633 - val_loss: 0.7037 - val_acc: 0.7310\n",
            "\n",
            "Epoch 00300: val_loss improved from 0.70568 to 0.70374, saving model to subj1.hdf5\n",
            "Epoch 301/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9164 - acc: 0.6530 - val_loss: 0.7234 - val_acc: 0.7262\n",
            "Epoch 302/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9170 - acc: 0.6540 - val_loss: 0.7210 - val_acc: 0.7250\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.70374\n",
            "Epoch 303/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9012 - acc: 0.6591 - val_loss: 0.7120 - val_acc: 0.7256\n",
            "Epoch 304/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8862 - acc: 0.6641 - val_loss: 0.7041 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.70374\n",
            "Epoch 305/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9174 - acc: 0.6532 - val_loss: 0.7211 - val_acc: 0.7236\n",
            "Epoch 306/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9153 - acc: 0.6520 - val_loss: 0.7212 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.70374\n",
            "Epoch 307/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9046 - acc: 0.6572 - val_loss: 0.7154 - val_acc: 0.7280\n",
            "Epoch 308/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8871 - acc: 0.6629 - val_loss: 0.7026 - val_acc: 0.7332\n",
            "\n",
            "Epoch 00308: val_loss improved from 0.70374 to 0.70257, saving model to subj1.hdf5\n",
            "Epoch 309/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9136 - acc: 0.6541 - val_loss: 0.7273 - val_acc: 0.7199\n",
            "Epoch 310/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9176 - acc: 0.6540 - val_loss: 0.7214 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.70257\n",
            "Epoch 311/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8992 - acc: 0.6593 - val_loss: 0.7107 - val_acc: 0.7293\n",
            "Epoch 312/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8864 - acc: 0.6644 - val_loss: 0.7039 - val_acc: 0.7320\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.70257\n",
            "Epoch 313/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9151 - acc: 0.6529 - val_loss: 0.7305 - val_acc: 0.7202\n",
            "Epoch 314/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9155 - acc: 0.6534 - val_loss: 0.7236 - val_acc: 0.7226\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.70257\n",
            "Epoch 315/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8982 - acc: 0.6590 - val_loss: 0.7102 - val_acc: 0.7300\n",
            "Epoch 316/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8851 - acc: 0.6642 - val_loss: 0.7036 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.70257\n",
            "Epoch 317/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9142 - acc: 0.6545 - val_loss: 0.7328 - val_acc: 0.7216\n",
            "Epoch 318/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9183 - acc: 0.6532 - val_loss: 0.7275 - val_acc: 0.7233\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.70257\n",
            "Epoch 319/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9021 - acc: 0.6576 - val_loss: 0.7121 - val_acc: 0.7284\n",
            "Epoch 320/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8854 - acc: 0.6636 - val_loss: 0.7019 - val_acc: 0.7328\n",
            "\n",
            "Epoch 00320: val_loss improved from 0.70257 to 0.70188, saving model to subj1.hdf5\n",
            "Epoch 321/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9131 - acc: 0.6543 - val_loss: 0.7215 - val_acc: 0.7251\n",
            "Epoch 322/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9122 - acc: 0.6537 - val_loss: 0.7204 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.70188\n",
            "Epoch 323/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9001 - acc: 0.6576 - val_loss: 0.7072 - val_acc: 0.7295\n",
            "Epoch 324/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8868 - acc: 0.6635 - val_loss: 0.7019 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.70188\n",
            "Epoch 325/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9122 - acc: 0.6545 - val_loss: 0.7283 - val_acc: 0.7237\n",
            "Epoch 326/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9251 - acc: 0.6507 - val_loss: 0.7208 - val_acc: 0.7263\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.70188\n",
            "Epoch 327/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9042 - acc: 0.6576 - val_loss: 0.7121 - val_acc: 0.7285\n",
            "Epoch 328/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8869 - acc: 0.6626 - val_loss: 0.7045 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.70188\n",
            "Epoch 329/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9127 - acc: 0.6539 - val_loss: 0.7363 - val_acc: 0.7178\n",
            "Epoch 330/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9172 - acc: 0.6526 - val_loss: 0.7191 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.70188\n",
            "Epoch 331/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8968 - acc: 0.6601 - val_loss: 0.7113 - val_acc: 0.7276\n",
            "Epoch 332/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8876 - acc: 0.6636 - val_loss: 0.7028 - val_acc: 0.7323\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.70188\n",
            "Epoch 333/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9117 - acc: 0.6543 - val_loss: 0.7206 - val_acc: 0.7252\n",
            "Epoch 334/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9131 - acc: 0.6544 - val_loss: 0.7220 - val_acc: 0.7235\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.70188\n",
            "Epoch 335/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8966 - acc: 0.6600 - val_loss: 0.7084 - val_acc: 0.7298\n",
            "Epoch 336/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8848 - acc: 0.6649 - val_loss: 0.7025 - val_acc: 0.7326\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.70188\n",
            "Epoch 337/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9110 - acc: 0.6563 - val_loss: 0.7254 - val_acc: 0.7204\n",
            "Epoch 338/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9101 - acc: 0.6555 - val_loss: 0.7186 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.70188\n",
            "Epoch 339/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8988 - acc: 0.6586 - val_loss: 0.7079 - val_acc: 0.7310\n",
            "Epoch 340/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8838 - acc: 0.6637 - val_loss: 0.6992 - val_acc: 0.7343\n",
            "\n",
            "Epoch 00340: val_loss improved from 0.70188 to 0.69923, saving model to subj1.hdf5\n",
            "Epoch 341/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9092 - acc: 0.6549 - val_loss: 0.7179 - val_acc: 0.7275\n",
            "Epoch 342/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9135 - acc: 0.6524 - val_loss: 0.7219 - val_acc: 0.7204\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.69923\n",
            "Epoch 343/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8970 - acc: 0.6594 - val_loss: 0.7081 - val_acc: 0.7288\n",
            "Epoch 344/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8831 - acc: 0.6653 - val_loss: 0.6998 - val_acc: 0.7348\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.69923\n",
            "Epoch 345/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9064 - acc: 0.6561 - val_loss: 0.7187 - val_acc: 0.7284\n",
            "Epoch 346/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9092 - acc: 0.6562 - val_loss: 0.7196 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.69923\n",
            "Epoch 347/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8952 - acc: 0.6610 - val_loss: 0.7091 - val_acc: 0.7288\n",
            "Epoch 348/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8800 - acc: 0.6657 - val_loss: 0.6988 - val_acc: 0.7341\n",
            "\n",
            "Epoch 00348: val_loss improved from 0.69923 to 0.69876, saving model to subj1.hdf5\n",
            "Epoch 349/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9060 - acc: 0.6560 - val_loss: 0.7221 - val_acc: 0.7266\n",
            "Epoch 350/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9062 - acc: 0.6563 - val_loss: 0.7125 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.69876\n",
            "Epoch 351/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8950 - acc: 0.6612 - val_loss: 0.7034 - val_acc: 0.7319\n",
            "Epoch 352/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8810 - acc: 0.6664 - val_loss: 0.6979 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00352: val_loss improved from 0.69876 to 0.69788, saving model to subj1.hdf5\n",
            "Epoch 353/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9096 - acc: 0.6556 - val_loss: 0.7195 - val_acc: 0.7255\n",
            "Epoch 354/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9091 - acc: 0.6568 - val_loss: 0.7172 - val_acc: 0.7223\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.69788\n",
            "Epoch 355/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8952 - acc: 0.6611 - val_loss: 0.7066 - val_acc: 0.7295\n",
            "Epoch 356/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8804 - acc: 0.6648 - val_loss: 0.6985 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.69788\n",
            "Epoch 357/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9073 - acc: 0.6570 - val_loss: 0.7227 - val_acc: 0.7215\n",
            "Epoch 358/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9109 - acc: 0.6546 - val_loss: 0.7171 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.69788\n",
            "Epoch 359/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8938 - acc: 0.6600 - val_loss: 0.7030 - val_acc: 0.7320\n",
            "Epoch 360/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8791 - acc: 0.6657 - val_loss: 0.6964 - val_acc: 0.7341\n",
            "\n",
            "Epoch 00360: val_loss improved from 0.69788 to 0.69644, saving model to subj1.hdf5\n",
            "Epoch 361/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.9057 - acc: 0.6566 - val_loss: 0.7204 - val_acc: 0.7252\n",
            "Epoch 362/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9076 - acc: 0.6568 - val_loss: 0.7117 - val_acc: 0.7260\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.69644\n",
            "Epoch 363/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8940 - acc: 0.6611 - val_loss: 0.7043 - val_acc: 0.7299\n",
            "Epoch 364/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8804 - acc: 0.6671 - val_loss: 0.6979 - val_acc: 0.7330\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.69644\n",
            "Epoch 365/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9078 - acc: 0.6549 - val_loss: 0.7199 - val_acc: 0.7242\n",
            "Epoch 366/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9057 - acc: 0.6573 - val_loss: 0.7159 - val_acc: 0.7238\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.69644\n",
            "Epoch 367/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8954 - acc: 0.6600 - val_loss: 0.7030 - val_acc: 0.7339\n",
            "Epoch 368/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8803 - acc: 0.6649 - val_loss: 0.6969 - val_acc: 0.7347\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.69644\n",
            "Epoch 369/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9087 - acc: 0.6549 - val_loss: 0.7172 - val_acc: 0.7254\n",
            "Epoch 370/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9077 - acc: 0.6573 - val_loss: 0.7162 - val_acc: 0.7247\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.69644\n",
            "Epoch 371/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8971 - acc: 0.6585 - val_loss: 0.7065 - val_acc: 0.7311\n",
            "Epoch 372/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8768 - acc: 0.6676 - val_loss: 0.6952 - val_acc: 0.7363\n",
            "\n",
            "Epoch 00372: val_loss improved from 0.69644 to 0.69516, saving model to subj1.hdf5\n",
            "Epoch 373/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9051 - acc: 0.6567 - val_loss: 0.7161 - val_acc: 0.7283\n",
            "Epoch 374/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9069 - acc: 0.6557 - val_loss: 0.7150 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.69516\n",
            "Epoch 375/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8928 - acc: 0.6610 - val_loss: 0.7034 - val_acc: 0.7339\n",
            "Epoch 376/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8802 - acc: 0.6662 - val_loss: 0.6961 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.69516\n",
            "Epoch 377/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9032 - acc: 0.6581 - val_loss: 0.7227 - val_acc: 0.7245\n",
            "Epoch 378/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9094 - acc: 0.6557 - val_loss: 0.7125 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.69516\n",
            "Epoch 379/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8911 - acc: 0.6612 - val_loss: 0.7039 - val_acc: 0.7297\n",
            "Epoch 380/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8779 - acc: 0.6682 - val_loss: 0.6946 - val_acc: 0.7364\n",
            "\n",
            "Epoch 00380: val_loss improved from 0.69516 to 0.69460, saving model to subj1.hdf5\n",
            "Epoch 381/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9036 - acc: 0.6585 - val_loss: 0.7105 - val_acc: 0.7275\n",
            "Epoch 382/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9074 - acc: 0.6559 - val_loss: 0.7135 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.69460\n",
            "Epoch 383/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8911 - acc: 0.6619 - val_loss: 0.7042 - val_acc: 0.7302\n",
            "Epoch 384/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8804 - acc: 0.6655 - val_loss: 0.6940 - val_acc: 0.7373\n",
            "\n",
            "Epoch 00384: val_loss improved from 0.69460 to 0.69397, saving model to subj1.hdf5\n",
            "Epoch 385/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9053 - acc: 0.6571 - val_loss: 0.7187 - val_acc: 0.7262\n",
            "Epoch 386/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9081 - acc: 0.6556 - val_loss: 0.7130 - val_acc: 0.7276\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.69397\n",
            "Epoch 387/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8929 - acc: 0.6624 - val_loss: 0.7035 - val_acc: 0.7324\n",
            "Epoch 388/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8755 - acc: 0.6678 - val_loss: 0.6932 - val_acc: 0.7378\n",
            "\n",
            "Epoch 00388: val_loss improved from 0.69397 to 0.69322, saving model to subj1.hdf5\n",
            "Epoch 389/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9076 - acc: 0.6569 - val_loss: 0.7136 - val_acc: 0.7273\n",
            "Epoch 390/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9062 - acc: 0.6577 - val_loss: 0.7170 - val_acc: 0.7273\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.69322\n",
            "Epoch 391/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8881 - acc: 0.6634 - val_loss: 0.6999 - val_acc: 0.7341\n",
            "Epoch 392/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8757 - acc: 0.6676 - val_loss: 0.6939 - val_acc: 0.7374\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.69322\n",
            "Epoch 393/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9059 - acc: 0.6566 - val_loss: 0.7179 - val_acc: 0.7259\n",
            "Epoch 394/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9047 - acc: 0.6571 - val_loss: 0.7093 - val_acc: 0.7310\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.69322\n",
            "Epoch 395/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8907 - acc: 0.6612 - val_loss: 0.7027 - val_acc: 0.7321\n",
            "Epoch 396/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8770 - acc: 0.6655 - val_loss: 0.6959 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.69322\n",
            "Epoch 397/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9010 - acc: 0.6570 - val_loss: 0.7126 - val_acc: 0.7277\n",
            "Epoch 398/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9035 - acc: 0.6570 - val_loss: 0.7121 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.69322\n",
            "Epoch 399/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8861 - acc: 0.6629 - val_loss: 0.7029 - val_acc: 0.7326\n",
            "Epoch 400/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8734 - acc: 0.6686 - val_loss: 0.6930 - val_acc: 0.7352\n",
            "\n",
            "Epoch 00400: val_loss improved from 0.69322 to 0.69302, saving model to subj1.hdf5\n",
            "Epoch 401/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9046 - acc: 0.6570 - val_loss: 0.7143 - val_acc: 0.7260\n",
            "Epoch 402/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9000 - acc: 0.6591 - val_loss: 0.7092 - val_acc: 0.7293\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.69302\n",
            "Epoch 403/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8905 - acc: 0.6628 - val_loss: 0.6981 - val_acc: 0.7350\n",
            "Epoch 404/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8778 - acc: 0.6664 - val_loss: 0.6922 - val_acc: 0.7377\n",
            "\n",
            "Epoch 00404: val_loss improved from 0.69302 to 0.69218, saving model to subj1.hdf5\n",
            "Epoch 405/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9032 - acc: 0.6578 - val_loss: 0.7166 - val_acc: 0.7261\n",
            "Epoch 406/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9035 - acc: 0.6582 - val_loss: 0.7106 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.69218\n",
            "Epoch 407/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8918 - acc: 0.6615 - val_loss: 0.7003 - val_acc: 0.7324\n",
            "Epoch 408/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8746 - acc: 0.6667 - val_loss: 0.6926 - val_acc: 0.7364\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.69218\n",
            "Epoch 409/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9019 - acc: 0.6578 - val_loss: 0.7153 - val_acc: 0.7257\n",
            "Epoch 410/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9017 - acc: 0.6590 - val_loss: 0.7094 - val_acc: 0.7313\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.69218\n",
            "Epoch 411/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8863 - acc: 0.6635 - val_loss: 0.6993 - val_acc: 0.7342\n",
            "Epoch 412/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8761 - acc: 0.6672 - val_loss: 0.6930 - val_acc: 0.7375\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.69218\n",
            "Epoch 413/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9007 - acc: 0.6585 - val_loss: 0.7123 - val_acc: 0.7276\n",
            "Epoch 414/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9028 - acc: 0.6577 - val_loss: 0.7098 - val_acc: 0.7252\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.69218\n",
            "Epoch 415/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8896 - acc: 0.6625 - val_loss: 0.6998 - val_acc: 0.7319\n",
            "Epoch 416/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8744 - acc: 0.6679 - val_loss: 0.6934 - val_acc: 0.7351\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.69218\n",
            "Epoch 417/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9044 - acc: 0.6572 - val_loss: 0.7146 - val_acc: 0.7262\n",
            "Epoch 418/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8983 - acc: 0.6589 - val_loss: 0.7091 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.69218\n",
            "Epoch 419/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8860 - acc: 0.6642 - val_loss: 0.7005 - val_acc: 0.7335\n",
            "Epoch 420/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8759 - acc: 0.6669 - val_loss: 0.6912 - val_acc: 0.7363\n",
            "\n",
            "Epoch 00420: val_loss improved from 0.69218 to 0.69123, saving model to subj1.hdf5\n",
            "Epoch 421/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9034 - acc: 0.6575 - val_loss: 0.7130 - val_acc: 0.7272\n",
            "Epoch 422/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9007 - acc: 0.6597 - val_loss: 0.7108 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.69123\n",
            "Epoch 423/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8901 - acc: 0.6613 - val_loss: 0.7025 - val_acc: 0.7314\n",
            "Epoch 424/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8711 - acc: 0.6681 - val_loss: 0.6923 - val_acc: 0.7368\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.69123\n",
            "Epoch 425/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8988 - acc: 0.6590 - val_loss: 0.7158 - val_acc: 0.7266\n",
            "Epoch 426/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9023 - acc: 0.6569 - val_loss: 0.7100 - val_acc: 0.7283\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.69123\n",
            "Epoch 427/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8858 - acc: 0.6633 - val_loss: 0.6965 - val_acc: 0.7351\n",
            "Epoch 428/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8719 - acc: 0.6701 - val_loss: 0.6922 - val_acc: 0.7366\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.69123\n",
            "Epoch 429/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9013 - acc: 0.6587 - val_loss: 0.7133 - val_acc: 0.7254\n",
            "Epoch 430/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8990 - acc: 0.6595 - val_loss: 0.7102 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.69123\n",
            "Epoch 431/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8857 - acc: 0.6646 - val_loss: 0.6994 - val_acc: 0.7317\n",
            "Epoch 432/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8739 - acc: 0.6689 - val_loss: 0.6907 - val_acc: 0.7371\n",
            "\n",
            "Epoch 00432: val_loss improved from 0.69123 to 0.69072, saving model to subj1.hdf5\n",
            "Epoch 433/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9004 - acc: 0.6596 - val_loss: 0.7177 - val_acc: 0.7259\n",
            "Epoch 434/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.9015 - acc: 0.6592 - val_loss: 0.7107 - val_acc: 0.7276\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.69072\n",
            "Epoch 435/500\n",
            "159042/159042 [==============================] - 4s 22us/step - loss: 0.8860 - acc: 0.6646 - val_loss: 0.6987 - val_acc: 0.7317\n",
            "Epoch 436/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8755 - acc: 0.6678 - val_loss: 0.6918 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.69072\n",
            "Epoch 437/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8985 - acc: 0.6586 - val_loss: 0.7121 - val_acc: 0.7285\n",
            "Epoch 438/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8972 - acc: 0.6596 - val_loss: 0.7025 - val_acc: 0.7310\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.69072\n",
            "Epoch 439/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8880 - acc: 0.6633 - val_loss: 0.6991 - val_acc: 0.7332\n",
            "Epoch 440/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8729 - acc: 0.6682 - val_loss: 0.6903 - val_acc: 0.7359\n",
            "\n",
            "Epoch 00440: val_loss improved from 0.69072 to 0.69028, saving model to subj1.hdf5\n",
            "Epoch 441/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8965 - acc: 0.6594 - val_loss: 0.7128 - val_acc: 0.7244\n",
            "Epoch 442/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8990 - acc: 0.6595 - val_loss: 0.7146 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.69028\n",
            "Epoch 443/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8867 - acc: 0.6649 - val_loss: 0.6990 - val_acc: 0.7332\n",
            "Epoch 444/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8742 - acc: 0.6689 - val_loss: 0.6913 - val_acc: 0.7366\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.69028\n",
            "Epoch 445/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8965 - acc: 0.6601 - val_loss: 0.7077 - val_acc: 0.7305\n",
            "Epoch 446/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.9001 - acc: 0.6576 - val_loss: 0.7066 - val_acc: 0.7287\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.69028\n",
            "Epoch 447/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8855 - acc: 0.6651 - val_loss: 0.6970 - val_acc: 0.7354\n",
            "Epoch 448/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8718 - acc: 0.6703 - val_loss: 0.6900 - val_acc: 0.7368\n",
            "\n",
            "Epoch 00448: val_loss improved from 0.69028 to 0.69001, saving model to subj1.hdf5\n",
            "Epoch 449/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8976 - acc: 0.6599 - val_loss: 0.7129 - val_acc: 0.7271\n",
            "Epoch 450/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8943 - acc: 0.6601 - val_loss: 0.7071 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.69001\n",
            "Epoch 451/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8850 - acc: 0.6646 - val_loss: 0.6993 - val_acc: 0.7330\n",
            "Epoch 452/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8674 - acc: 0.6696 - val_loss: 0.6860 - val_acc: 0.7387\n",
            "\n",
            "Epoch 00452: val_loss improved from 0.69001 to 0.68596, saving model to subj1.hdf5\n",
            "Epoch 453/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8990 - acc: 0.6599 - val_loss: 0.7117 - val_acc: 0.7299\n",
            "Epoch 454/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8963 - acc: 0.6597 - val_loss: 0.7075 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.68596\n",
            "Epoch 455/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8854 - acc: 0.6644 - val_loss: 0.6956 - val_acc: 0.7368\n",
            "Epoch 456/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8725 - acc: 0.6677 - val_loss: 0.6879 - val_acc: 0.7384\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.68596\n",
            "Epoch 457/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8950 - acc: 0.6611 - val_loss: 0.7076 - val_acc: 0.7321\n",
            "Epoch 458/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8977 - acc: 0.6588 - val_loss: 0.7066 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.68596\n",
            "Epoch 459/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8848 - acc: 0.6639 - val_loss: 0.6964 - val_acc: 0.7341\n",
            "Epoch 460/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8699 - acc: 0.6691 - val_loss: 0.6885 - val_acc: 0.7378\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.68596\n",
            "Epoch 461/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8956 - acc: 0.6602 - val_loss: 0.7099 - val_acc: 0.7303\n",
            "Epoch 462/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8964 - acc: 0.6606 - val_loss: 0.7046 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.68596\n",
            "Epoch 463/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8829 - acc: 0.6655 - val_loss: 0.6966 - val_acc: 0.7339\n",
            "Epoch 464/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8714 - acc: 0.6691 - val_loss: 0.6876 - val_acc: 0.7382\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.68596\n",
            "Epoch 465/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8961 - acc: 0.6595 - val_loss: 0.7057 - val_acc: 0.7311\n",
            "Epoch 466/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8976 - acc: 0.6590 - val_loss: 0.7034 - val_acc: 0.7320\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.68596\n",
            "Epoch 467/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8862 - acc: 0.6637 - val_loss: 0.6975 - val_acc: 0.7357\n",
            "Epoch 468/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8723 - acc: 0.6685 - val_loss: 0.6889 - val_acc: 0.7374\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.68596\n",
            "Epoch 469/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8982 - acc: 0.6595 - val_loss: 0.7090 - val_acc: 0.7286\n",
            "Epoch 470/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8961 - acc: 0.6598 - val_loss: 0.7102 - val_acc: 0.7257\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.68596\n",
            "Epoch 471/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8839 - acc: 0.6639 - val_loss: 0.6978 - val_acc: 0.7314\n",
            "Epoch 472/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8716 - acc: 0.6692 - val_loss: 0.6890 - val_acc: 0.7375\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.68596\n",
            "Epoch 473/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8972 - acc: 0.6605 - val_loss: 0.7121 - val_acc: 0.7305\n",
            "Epoch 474/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8969 - acc: 0.6601 - val_loss: 0.7059 - val_acc: 0.7288\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.68596\n",
            "Epoch 475/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8811 - acc: 0.6662 - val_loss: 0.6978 - val_acc: 0.7343\n",
            "Epoch 476/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8728 - acc: 0.6680 - val_loss: 0.6870 - val_acc: 0.7391\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.68596\n",
            "Epoch 477/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8951 - acc: 0.6602 - val_loss: 0.7099 - val_acc: 0.7268\n",
            "Epoch 478/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8932 - acc: 0.6614 - val_loss: 0.7023 - val_acc: 0.7309\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.68596\n",
            "Epoch 479/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8845 - acc: 0.6643 - val_loss: 0.6945 - val_acc: 0.7366\n",
            "Epoch 480/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8690 - acc: 0.6700 - val_loss: 0.6898 - val_acc: 0.7375\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.68596\n",
            "Epoch 481/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8948 - acc: 0.6604 - val_loss: 0.7073 - val_acc: 0.7292\n",
            "Epoch 482/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8961 - acc: 0.6601 - val_loss: 0.7049 - val_acc: 0.7309\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.68596\n",
            "Epoch 483/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8829 - acc: 0.6652 - val_loss: 0.6964 - val_acc: 0.7334\n",
            "Epoch 484/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8696 - acc: 0.6701 - val_loss: 0.6860 - val_acc: 0.7400\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.68596\n",
            "Epoch 485/500\n",
            "159042/159042 [==============================] - 3s 21us/step - loss: 0.8941 - acc: 0.6600 - val_loss: 0.7058 - val_acc: 0.7310\n",
            "Epoch 486/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8936 - acc: 0.6599 - val_loss: 0.7077 - val_acc: 0.7288\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.68596\n",
            "Epoch 487/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8790 - acc: 0.6654 - val_loss: 0.6909 - val_acc: 0.7348\n",
            "Epoch 488/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8678 - acc: 0.6704 - val_loss: 0.6830 - val_acc: 0.7403\n",
            "\n",
            "Epoch 00488: val_loss improved from 0.68596 to 0.68296, saving model to subj1.hdf5\n",
            "Epoch 489/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8913 - acc: 0.6620 - val_loss: 0.7057 - val_acc: 0.7304\n",
            "Epoch 490/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8921 - acc: 0.6615 - val_loss: 0.7016 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.68296\n",
            "Epoch 491/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8821 - acc: 0.6644 - val_loss: 0.6927 - val_acc: 0.7359\n",
            "Epoch 492/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8700 - acc: 0.6706 - val_loss: 0.6853 - val_acc: 0.7392\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.68296\n",
            "Epoch 493/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8949 - acc: 0.6612 - val_loss: 0.7047 - val_acc: 0.7321\n",
            "Epoch 494/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8950 - acc: 0.6611 - val_loss: 0.7026 - val_acc: 0.7328\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.68296\n",
            "Epoch 495/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8807 - acc: 0.6662 - val_loss: 0.6901 - val_acc: 0.7354\n",
            "Epoch 496/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8672 - acc: 0.6717 - val_loss: 0.6850 - val_acc: 0.7409\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.68296\n",
            "Epoch 497/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8957 - acc: 0.6614 - val_loss: 0.7054 - val_acc: 0.7324\n",
            "Epoch 498/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8964 - acc: 0.6612 - val_loss: 0.7023 - val_acc: 0.7285\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.68296\n",
            "Epoch 499/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8814 - acc: 0.6666 - val_loss: 0.6933 - val_acc: 0.7345\n",
            "Epoch 500/500\n",
            "159042/159042 [==============================] - 3s 22us/step - loss: 0.8685 - acc: 0.6701 - val_loss: 0.6863 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.68296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0283ddb3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "09CHoZ2fC_2S",
        "colab_type": "code",
        "outputId": "5df1bbcc-789f-4b42-8269-e4df5ef2395b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_val, verbose = 1) ; preds.shape\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33049/33049 [==============================] - 4s 129us/step\n",
            "[[3.39404394e-13 9.93124902e-01 5.23311552e-03 ... 9.34189970e-09\n",
            "  2.24512542e-06 5.11171929e-05]\n",
            " [5.43816334e-08 4.41716518e-03 6.81430250e-02 ... 6.00301973e-07\n",
            "  9.17529064e-07 9.41763290e-07]\n",
            " [1.36317915e-10 1.24232165e-05 1.12054136e-06 ... 1.23332240e-01\n",
            "  1.37482986e-01 7.39169300e-01]\n",
            " ...\n",
            " [1.08133129e-08 1.02636106e-02 3.00107840e-02 ... 2.20132605e-08\n",
            "  4.60885596e-09 5.32618643e-08]\n",
            " [6.19726848e-08 9.12420655e-05 4.08950364e-05 ... 1.70677438e-01\n",
            "  6.05338454e-01 2.23590016e-01]\n",
            " [1.07251447e-14 9.99502182e-01 4.46951046e-04 ... 4.28398705e-10\n",
            "  1.77677439e-09 1.07892335e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KrqRep9hDiDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_classes = preds.argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7Ydxe_6Eh-Q",
        "colab_type": "code",
        "outputId": "c49d51b3-a4fd-4d12-a247-c32542eee3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_val_classes = y_val.argmax(axis = -1)\n",
        "print(y_val_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 5 9 ... 4 9 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-WcSgOBEZ02",
        "colab_type": "code",
        "outputId": "bd1238f3-86f7-46f0-c524-0a3afa94d997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "y_classes\n",
        "br_f1=metrics.f1_score(y_val_classes, y_classes, average='micro')\n",
        "print(br_f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.704711186420164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i8fxCJ3htGUc",
        "colab_type": "code",
        "outputId": "fdd691da-0580-4661-d762-fe4da2437946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "result = clf.predict(x_val)\n",
        "result = result.toarray()\n",
        "\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "for index in range(6):\n",
        "  fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val[:,index], result[:,index])\n",
        "  auc_keras = auc(fpr_keras, tpr_keras)\n",
        "  print(auc_keras)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-83d020e951ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bwfqg6ywcOlg",
        "colab_type": "code",
        "outputId": "5516804e-ae7f-4a5d-8b35-675ffd56d741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "for i in range(6):\n",
        "  y_test = y_val[:,i]\n",
        "  y_hat = result[:,i]\n",
        "  br_f1=metrics.f1_score(y_test, y_hat, average='micro')\n",
        "  br_hamm=metrics.hamming_loss(y_test,y_hat)\n",
        "  print('Binary Relevance F1-score:',round(br_f1,3))\n",
        "  print('Binary Relevance Hamming Loss:',round(br_hamm,3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary Relevance F1-score: 0.834\n",
            "Binary Relevance Hamming Loss: 0.166\n",
            "Binary Relevance F1-score: 0.812\n",
            "Binary Relevance Hamming Loss: 0.188\n",
            "Binary Relevance F1-score: 0.799\n",
            "Binary Relevance Hamming Loss: 0.201\n",
            "Binary Relevance F1-score: 0.774\n",
            "Binary Relevance Hamming Loss: 0.226\n",
            "Binary Relevance F1-score: 0.874\n",
            "Binary Relevance Hamming Loss: 0.126\n",
            "Binary Relevance F1-score: 0.859\n",
            "Binary Relevance Hamming Loss: 0.141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KBs4zCLKdxTA",
        "colab_type": "code",
        "outputId": "5e4b26d1-3339-4fea-9083-4b5f7898d244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(298085, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6dwJpt-OeBzU",
        "colab_type": "code",
        "outputId": "7e68867f-d7fd-4da8-bd4b-c19e9bb371fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d81d1eea2757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollinear_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'collinear_features' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iJz5aNQWSMus",
        "colab_type": "code",
        "outputId": "baa681d1-3526-4d70-c65a-ac72e4b0b413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1034
        }
      },
      "cell_type": "code",
      "source": [
        "start=time.time()\n",
        "final_preds=clf.predict(X_test)\n",
        "print('prediction time taken: ',round(time.time()-start,0),'seconds')\n",
        "final_preds = final_preds.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-0b4a68d1ab97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction time taken: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skmultilearn/problem_transform/br.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    180\u001b[0m         predictions = [self._ensure_multi_label_from_single_class(\n\u001b[1;32m    181\u001b[0m             self.classifiers_[label].predict(self._ensure_input_format(X)))\n\u001b[0;32m--> 182\u001b[0;31m             for label in range(self.model_count_)]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skmultilearn/problem_transform/br.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m         predictions = [self._ensure_multi_label_from_single_class(\n\u001b[1;32m    181\u001b[0m             self.classifiers_[label].predict(self._ensure_input_format(X)))\n\u001b[0;32m--> 182\u001b[0;31m             for label in range(self.model_count_)]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skmultilearn/ext/keras.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_22 to have shape (26,) but got array with shape (32,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zQjABKd2ySZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR8UaYCxqp2o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The all zero target, ie (0,0,0,0,0,0) makes up 87.44% of the data, so it's pretty imbalanced.\n"
      ]
    },
    {
      "metadata": {
        "id": "RFY9YKhyOiiW",
        "colab_type": "code",
        "outputId": "ce4debb2-28e0-4d59-9a3b-2309d805901d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(subject)\n",
        "\n",
        "# submission file\n",
        "cols = ['HandStart','FirstDigitTouch',\n",
        "        'BothStartLoadPhase','LiftOff',\n",
        "        'Replace','BothReleased']\n",
        "submission_file = 'TCN_%d_2stacks.csv'\n",
        "# create pandas object for submission\n",
        "submission = pd.DataFrame(index = ids,\n",
        "                          columns = cols,\n",
        "                          data = final_preds)\n",
        "\n",
        "# write file\n",
        "submission.to_csv(submission_file,index_label='id',float_format='%.3f')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2Hdk3KuOiiB",
        "colab_type": "code",
        "outputId": "6ec78761-bc6d-4a5b-fcfb-31f4fcaf0d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp /content/TCN_2_2stacks.csv gs://peijinbucket"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: No URLs matched: /content/TCN_2_2stacks.csv\n",
            "CommandException: 1 file/object could not be transferred.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlfA4KSqLsk_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cea9VQ_1pyxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KNp0vq1TXbwF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ng2UO4e_ZQGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpdzGpqMaLU7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKBpEAFAak-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FuYI07v0VKpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "0WkSwr7LOiiQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lT3Fvrftuo20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CRqbTkAsI1uH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ids2 = ids.copy()\n",
        "#skup to ithe following cell\n",
        "ids3 = ids.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nV-ntoiHS-FF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ids_total = np.concatenate([ids2,ids3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpRL__yBTBq6",
        "colab_type": "code",
        "outputId": "c8f0e033-ee2a-48d9-b911-324dda6656b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'test',\n",
              " 'adc.json',\n",
              " 'eeg_train',\n",
              " 'newTCN4.csv',\n",
              " 'newTCN3.csv',\n",
              " 'OpenNE',\n",
              " 'newTCN2.csv',\n",
              " 'newTCN1.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "bJcVlcu-ad7O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# See This "
      ]
    },
    {
      "metadata": {
        "id": "HK4-MXehYjTy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "urKrv4hKNxuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_gen = BalancedBatchGenerator(trainX, trainY, sampler=RandomOverSampler(sampling_strategy = \"minority\"), batch_size = 1100, random_state=42)\n",
        "val_gen = BalancedBatchGenerator(valX, valY, sampler=RandomOverSampler(sampling_strategy = \"minority\"), batch_size = 1100, random_state=42)\n",
        "model.fit_generator(train_gen, val_gen, steps_per_epoch = ,epochs = 20, verbose = 1, callbacks = [checkpointer, reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDHbkPKWOiiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "raw = [] \n",
        "fnames =  glob('lstm*.csv')\n",
        "print(fnames)\n",
        "print(len(fnames))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChSzH4V8Nbie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for fname in fnames:\n",
        "    dat = pd.read_csv(fname)\n",
        "    raw.append(dat)\n",
        "final_submission = pd.concat(raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbxj2EsWcB0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_submission.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1zjPM8tOih9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_submission.to_csv(\"LSTM_TCN_trainedonhalf.csv\", sep = \",\", header = True, index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOAAiJ1lHdgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp LSTM_TCN_trainedonhalf.csv gs://peijinbucket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLqAh2NzHuk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp rounded_final_submission.csv gs://peijinbucket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzO2PgOLG5_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "rounded_final = np.round(final_submission)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}