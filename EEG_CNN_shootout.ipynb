{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_CNN_shootout.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3catz/deeplearning_timeseries/blob/master/EEG_CNN_shootout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ax--B1rIbrD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Beginnings"
      ]
    },
    {
      "metadata": {
        "id": "oKZ_mY6RUvft",
        "colab_type": "code",
        "outputId": "a8960a86-a44a-4c23-c9d0-88a2365d8490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3608
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install liac-arff\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install keras-tcn\n",
        "!pip install scikit-multilearn\n",
        "!pip install MNE\n",
        "\n",
        "#from skmultilearn.base.problem_transformation import LabelPowerset\n",
        "!pip install networkx tensorflow\n",
        "!git clone https://github.com/thunlp/OpenNE/\n",
        "!pip install -e OpenNE/src\n",
        "!pip install pyts\n",
        "!pip install pyprep\n",
        "!pip install smote-variants\n",
        "\n",
        "!pip install pyhht \n",
        "!pip install EMD-signal\n",
        "import os \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting liac-arff\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: liac-arff\n",
            "Successfully installed liac-arff-2.4.0\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.20.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Collecting keras-tcn\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/f7/f584a9b82c7c7110165949b3e9f1f6e0859979589eaed2b2bd70747e723a/keras_tcn-2.6.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.7)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.11.0)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-2.6.7\n",
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting MNE\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/e5/f60f23e3ef10f5b1efa1aabb407f0b2fc64ed12ac166a6e1c16dd7db6e77/mne-0.17.1.tar.gz (6.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 6.2MB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: MNE\n",
            "  Building wheel for MNE (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/39/f9/f5/3e2fdef321f8e1f64061730c8ba2c81ad55c4b1b860b29d2dd\n",
            "Successfully built MNE\n",
            "Installing collected packages: MNE\n",
            "Successfully installed MNE-0.17.1\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
            "Cloning into 'OpenNE'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 537 (delta 0), reused 7 (delta 0), pack-reused 530\u001b[K\n",
            "Receiving objects: 100% (537/537), 14.93 MiB | 17.37 MiB/s, done.\n",
            "Resolving deltas: 100% (286/286), done.\n",
            "Obtaining file:///content/OpenNE/src\n",
            "Installing collected packages: openne\n",
            "  Running setup.py develop for openne\n",
            "Successfully installed openne\n",
            "Collecting pyts\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/40/0567f1984a1741e89d8187764187ede3e81195b2cb9e260b9918e3bdb4f0/pyts-0.8.0-py3-none-any.whl (82kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hCollecting numpy>=1.15.4scipy>=1.1.0scikit-learn>=0.20.1numba>=0.41.0 (from pyts)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 2.4MB/s \n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pyts\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.16.2 pyts-0.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprep\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/7e/c9c3205c0dfdf4e05537b7bd2a036bfc9f194adb5d31b310aac7ca3a2a05/pyprep-0.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mne>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from pyprep) (0.17.1)\n",
            "Requirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from pyprep) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from pyprep) (1.16.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from pyprep) (5.4.8)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyprep) (1.1.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.8.0->pyprep) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.8.0->pyprep) (0.22.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy->statsmodels>=0.8.0->pyprep) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels>=0.8.0->pyprep) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels>=0.8.0->pyprep) (2018.9)\n",
            "Installing collected packages: pyprep\n",
            "Successfully installed pyprep-0.2.3\n",
            "Collecting smote-variants\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/29/628e2b83eef74847b5e145e9e16d6fc2968dbedba6c3cba79fec75d81ff5/smote_variants-0.2.6-py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from smote-variants) (0.22.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from smote-variants) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from smote-variants) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from smote-variants) (0.20.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from smote-variants) (2.2.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from smote-variants) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smote-variants) (0.12.5)\n",
            "Collecting statistics (from smote-variants)\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/3a/ae99a15e65636559d936dd2159d75af1619491e8cb770859fbc8aa62cef6/statistics-1.0.3.5.tar.gz\n",
            "Collecting minisom (from smote-variants)\n",
            "  Downloading https://files.pythonhosted.org/packages/69/69/ebb77000e8d722ee4ca82e79fe1d4eff0d140e324c378981c7e6480e8911/MiniSom-2.1.5.tar.gz\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->smote-variants) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->smote-variants) (2.5.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->smote-variants) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->smote-variants) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->smote-variants) (1.0.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->smote-variants) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->smote-variants) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (3.7.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (0.7.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (1.13.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (0.33.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->smote-variants) (1.15.0)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from statistics->smote-variants) (0.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->smote-variants) (40.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->smote-variants) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->smote-variants) (0.14.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->smote-variants) (2.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->smote-variants) (5.1.3)\n",
            "Building wheels for collected packages: statistics, minisom\n",
            "  Building wheel for statistics (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/55/90/73aa7662bfb4565b567618547a275f01372a678ca92ecd64f3\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f8/32/ba/ca4264f6b795502698c8db8b07011a953e5fe34457a49c0860\n",
            "Successfully built statistics minisom\n",
            "Installing collected packages: statistics, minisom, smote-variants\n",
            "Successfully installed minisom-2.1.5 smote-variants-0.2.6 statistics-1.0.3.5\n",
            "Collecting pyhht\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/9b/b3d239463d0e19cc748e183fde3521285e2d6049895bf6fe62703488e093/pyhht-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pyhht) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyhht) (1.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyhht) (1.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyhht) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyhht) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyhht) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyhht) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyhht) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyhht) (40.8.0)\n",
            "Installing collected packages: pyhht\n",
            "Successfully installed pyhht-0.1.0\n",
            "Collecting EMD-signal\n",
            "  Downloading https://files.pythonhosted.org/packages/10/8d/74c8737f76478852510ef7f83f7431266fc100344c2f6eeaa27be7e938fb/EMD_signal-0.2.6-py2.py3-none-any.whl\n",
            "Collecting pathos>=0.2.1 (from EMD-signal)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/3e/391c9a3c639fc5ce7502ac16fde81dcc5508f2a9cc0d1acc650725400b52/pathos-0.2.3.tar.gz (162kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from EMD-signal) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from EMD-signal) (1.16.2)\n",
            "Collecting numpydoc (from EMD-signal)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Collecting ppft>=1.6.4.9 (from pathos>=0.2.1->EMD-signal)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/6c/16bdc13a8defc8ccab8b5c1a3dfb1331343b313f52984be0f4d6521eb92c/ppft-1.6.4.9.tar.gz (60kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from pathos>=0.2.1->EMD-signal) (0.2.9)\n",
            "Collecting pox>=0.2.5 (from pathos>=0.2.1->EMD-signal)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/4d/56db34aac51e9bb2005cf7fffabbd1d7a7d18b42fe6ccb05ffae571d356b/pox-0.2.5.tar.gz (109kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess>=0.70.7 in /usr/local/lib/python3.6/dist-packages (from pathos>=0.2.1->EMD-signal) (0.70.7)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc->EMD-signal) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc->EMD-signal) (2.10)\n",
            "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from ppft>=1.6.4.9->pathos>=0.2.1->EMD-signal) (1.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (19.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (2.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (40.8.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (0.14)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (1.1.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (1.2.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc->EMD-signal) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc->EMD-signal) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->sphinx>=1.2.3->numpydoc->EMD-signal) (2.3.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.2.3->numpydoc->EMD-signal) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.2.3->numpydoc->EMD-signal) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.2.3->numpydoc->EMD-signal) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.2.3->numpydoc->EMD-signal) (3.0.4)\n",
            "Requirement already satisfied: pytz>=0a in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.2.3->numpydoc->EMD-signal) (2018.9)\n",
            "Building wheels for collected packages: pathos, numpydoc, ppft, pox\n",
            "  Building wheel for pathos (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/62/8a/d9/6990d88e45a165383edd897f94a852613bc0779e0e7363ad78\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Building wheel for ppft (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a7/f0/ca/84ed99b9a36a387211d89101b281e0cac82b609751e3631fd4\n",
            "  Building wheel for pox (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ed/30/07/f2bdd0b1cc48fd897dc36be0f5575f768be09b4931301abaa7\n",
            "Successfully built pathos numpydoc ppft pox\n",
            "Installing collected packages: ppft, pox, pathos, numpydoc, EMD-signal\n",
            "Successfully installed EMD-signal-0.2.6 numpydoc-0.8.0 pathos-0.2.3 pox-0.2.5 ppft-1.6.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uGX1uPGRR06s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import ConvLSTM2D, Conv2D, SeparableConv2D, Dense, Flatten, Bidirectional, TimeDistributed, RepeatVector, LSTM, Dropout, GRU, Conv1D, Conv2D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import *\n",
        "from keras.constraints import max_norm\n",
        "from keras.optimizers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import seed\n",
        "from keras.layers import Concatenate, GlobalAveragePooling1D,Activation, Permute, Lambda, multiply, GlobalMaxPooling1D\n",
        "from keras.models import Input, Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Reshape, Activation\n",
        "from keras.layers import concatenate\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical \n",
        "\n",
        "from keras.layers import SeparableConv2D, DepthwiseConv2D, AveragePooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.constraints import max_norm\n",
        "from keras import backend as K\n",
        "\n",
        "from tcn import TCN\n",
        "from google.colab import files \n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(5888)\n",
        "\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.ext import Keras\n",
        "from imblearn.keras import BalancedBatchGenerator\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler \n",
        "from glob import glob\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import butter, lfilter\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter \n",
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from pyts.classification import (BOSSVSClassifier, SAXVSMClassifier,\n",
        " #                                KNNClassifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rydY9rHUOoWs",
        "colab_type": "code",
        "outputId": "4683a3eb-d0b7-44c1-ad6d-f507a31c2b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-28843b2fd02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'auth' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "41aYse16-1y4",
        "colab_type": "code",
        "outputId": "b373bbb4-7fe1-4acc-e7e2-0e76ba70a0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "cell_type": "code",
      "source": [
        "import os ; os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'tcn_take2_12.csv',\n",
              " 'test',\n",
              " 'tcn_take2_11.csv',\n",
              " 'eeg_train',\n",
              " 'eegnet_02.csv',\n",
              " 'eegnet_final.csv',\n",
              " 'eegnet_03.csv',\n",
              " 'eegnet_01.csv',\n",
              " 'adc.json',\n",
              " 'LSTM_final.csv',\n",
              " 'eegnet_04.csv',\n",
              " 'lstm',\n",
              " 'OpenNE',\n",
              " 'tcn_take2_10.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "metadata": {
        "id": "_BwtarmgYny0",
        "colab_type": "code",
        "outputId": "1ef38b7b-18e2-4bf6-9c2c-dddb170c97a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3757
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://peijinbucket/eeg_train /content\n",
        "!gsutil -m cp -r gs://peijinbucket/test/ /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://peijinbucket/eeg_train/subj10_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series3_events.csv...\n",
            "/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series4_events.csv...\n",
            "/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \r/ [0 files][    0.0 B/  1.6 GiB]                                                \rCopying gs://peijinbucket/eeg_train/subj10_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj10_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj11_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj12_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj1_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj2_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj3_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj4_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj5_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj6_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj7_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj8_series8_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series1_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series1_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series2_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series2_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series3_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series3_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series4_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series4_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series5_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series5_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series6_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series6_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series7_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series7_events.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series8_data.csv...\n",
            "Copying gs://peijinbucket/eeg_train/subj9_series8_events.csv...\n",
            "/ [192/192 files][  3.1 GiB/  3.1 GiB] 100% Done 100.5 MiB/s ETA 00:00:00       \n",
            "Operation completed over 192 objects/3.1 GiB.                                    \n",
            "Copying gs://peijinbucket/test/subj10_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj10_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj11_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj1_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj12_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj11_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj1_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj2_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj2_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj12_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj3_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj3_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj4_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj4_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj5_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj5_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj6_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj6_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj7_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj7_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj8_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj8_series9_data.csv...\n",
            "Copying gs://peijinbucket/test/subj9_series10_data.csv...\n",
            "Copying gs://peijinbucket/test/subj9_series9_data.csv...\n",
            "| [24/24 files][446.3 MiB/446.3 MiB] 100% Done                                  \n",
            "Operation completed over 24 objects/446.3 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_l7Ok24Oihz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#############function to read data###########\n",
        "\n",
        "def prepare_data_train(fname):\n",
        "    \"\"\" read and prepare training data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    # events file\n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    # read event file\n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "#############function to read data###########\n",
        "\n",
        "def prepare_data_train(fname):\n",
        "    \"\"\" read and prepare training data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    # events file\n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    # read event file\n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "\n",
        "def prepare_data_test(fname):\n",
        "    \"\"\" read and prepare test data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    return data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "def data_preprocess_train(X):\n",
        "    X_prep=scaler.fit_transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n",
        "\n",
        "def data_preprocess_test(X):\n",
        "    X_prep=scaler.transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "grdsaj8djr4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del X_train, X_test, X, trainX, trainY, valX, valY, Xt, y, test_preds, submission, testdf, train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1sVV2CSbv3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading and preprocessing data "
      ]
    },
    {
      "metadata": {
        "id": "0Gzgi2OoOih4",
        "colab_type": "code",
        "outputId": "27cdea70-5c74-45c5-8c39-7282e178aa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "ids_tot = []\n",
        "pred_tot = []\n",
        "\n",
        "subj = [9]\n",
        "y_raw= []\n",
        "raw = []\n",
        "for subject in subj:\n",
        "  fnames =  glob('/content/eeg_train/subj%d_series*_data.csv' % (subject))\n",
        "  for fname in fnames:\n",
        "    data,labels=prepare_data_train(fname)\n",
        "    raw.append(data)\n",
        "    y_raw.append(labels)\n",
        "\n",
        "X = pd.concat(raw)\n",
        "y = pd.concat(y_raw)\n",
        "    #transform in numpy array\n",
        "    #transform train data in numpy array\n",
        "X_train = np.asarray(X.astype(np.float32))\n",
        "X_train = data_preprocess_train(X_train)\n",
        "y = np.asarray(y.astype(np.float32))\n",
        "\n",
        "\n",
        " \n",
        "test = []\n",
        "idx=[]\n",
        "for subject in subj:\n",
        "  fnames =  glob('/content/test/subj%d_series*_data.csv' % (subject))\n",
        "print(fnames)\n",
        "  \n",
        "for fname in fnames:\n",
        "  data=prepare_data_test(fname)\n",
        "  test.append(data)\n",
        "  idx.append(np.array(data['id']))\n",
        "X_test = pd.concat(test)\n",
        "ids = np.concatenate(idx)\n",
        "ids_tot.append(ids)\n",
        "X_test = X_test.drop(['id' ], axis=1)#remove id\n",
        "    #transform test data in numpy array\n",
        "\n",
        "X_test = data_preprocess_test(np.array(X_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/test/subj9_series10_data.csv', '/content/test/subj9_series9_data.csv']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cdiLrCrn4yg0",
        "colab_type": "code",
        "outputId": "35e3b9df-c70e-47cc-b4d1-38b6d3631286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "import os ; os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'adc.json',\n",
              " 'bob.npy',\n",
              " 'eeg_train',\n",
              " 'OpenNE',\n",
              " 'test',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "U5ZE1iqN8hZm",
        "colab_type": "code",
        "outputId": "4f6e7fd6-c209-4e7d-dd36-6a7fa6954670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape\n",
        "Xt = X_train[:,:].copy()\n",
        "\n",
        "Xt = Xt.astype(np.float64)\n",
        "Xt = np.transpose(Xt, (1,0))\n",
        "Xt.shape\n",
        "\n",
        "#X_beta = mne.filter.filter_data(data = Xt, sfreq=500, l_freq=18, h_freq=25, picks = None)\n",
        "#X_mu = mne.filter.filter_data(data = Xt, sfreq=500, l_freq=8, h_freq=12, picks = None)\n",
        "X_gen = mne.filter.filter_data(data = Xt, sfreq=500, l_freq=0.1, h_freq=40, picks = None)\n",
        "\n",
        "#X = np.concatenate([X_beta], axis = 0)\n",
        "X = X_gen \n",
        "X = np.transpose(X, (1,0))\n",
        "#X = X**2\n",
        "#pd.DataFrame(X)\n",
        "\n",
        "somescaler = StandardScaler()\n",
        "X = somescaler.fit_transform(X)\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up band-pass filter from 0.1 - 40 Hz\n",
            "l_trans_bandwidth chosen to be 0.1 Hz\n",
            "h_trans_bandwidth chosen to be 10.0 Hz\n",
            "Filter length of 16501 samples (33.002 sec) selected\n",
            "(1447494, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BG1wTUNcetEH",
        "colab_type": "code",
        "outputId": "f387e927-42b9-40ee-b3c8-a755fa6cb954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "trainX, valX, trainY, valY = train_test_split(X, y, test_size=0.30, random_state=3249, shuffle = False)\n",
        "print(trainX.shape, trainY.shape, valX.shape, valY.shape)\n",
        "\n",
        "train_gen = TimeseriesGenerator(trainX, trainY, length = 1000, \n",
        "                                stride = 50, \n",
        "                                sampling_rate = 10, \n",
        "                                shuffle = True, \n",
        "                                batch_size = 1000)\n",
        "val_gen =   TimeseriesGenerator(valX, valY, length = 1000,\n",
        "                                stride = 50,\n",
        "                                sampling_rate = 10, \n",
        "                                shuffle = True, \n",
        "                                batch_size = 1000)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1013245, 32) (1013245, 6) (434249, 32) (434249, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKTwYKs4YpPc",
        "colab_type": "code",
        "outputId": "ef5a5324-e3fd-42ea-eee1-5822839a2afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'adc.json',\n",
              " 'conv-lstm_02.csv',\n",
              " 'eeg_train',\n",
              " 'OpenNE',\n",
              " 'conv-lstm1',\n",
              " 'conv-lstm2',\n",
              " 'shallow_01.csv',\n",
              " 'conv-lstm_01.csv',\n",
              " 'test',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "okgnBaCsLEi7",
        "colab_type": "code",
        "outputId": "adebe71a-4858-482a-c0c7-30630dfea7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp /content/shallowmodel12 gs://peijinbucket/lstm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/shallowmodel12 [Content-Type=application/octet-stream]...\n",
            "/ [1/1 files][  3.6 MiB/  3.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/3.6 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2WXWawVi6Ib-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = 0.9, patience = 3, min_lr = 1e-4, verbose = 1)\n",
        "\n",
        "early = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 18, verbose = 1, restore_best_weights = True)\n",
        "\n",
        "#model_m = load_model('/content/conv-lstm%d' % 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LihMBRsohsLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_m.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3soUaefUZbq_",
        "colab_type": "code",
        "outputId": "0279bec6-36a1-4a35-cb54-209b211c1858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import AveragePooling1D\n",
        "model_m = Sequential()\n",
        "model_m.add(Dropout(0.50, input_shape=(100,32)))\n",
        "model_m.add(Conv1D(128, 4, kernel_constraint = max_norm(1.),padding = 'same'))\n",
        "model_m.add(Conv1D(128, 4, kernel_constraint = max_norm(1.),padding = 'same',))\n",
        "\n",
        "model_m.add(BatchNormalization(axis = -1))\n",
        "model_m.add(Activation(\"relu\"))\n",
        "model_m.add(MaxPooling1D(2))\n",
        "model_m.add(Dropout(0.50))\n",
        "\n",
        "model_m.add(Conv1D(128, 10, kernel_constraint = max_norm(1.),padding = 'same'))\n",
        "model_m.add(Conv1D(128, 10, kernel_constraint = max_norm(1.),padding = 'same'))\n",
        "model_m.add(BatchNormalization(axis=-1))\n",
        "model_m.add(Activation(\"relu\"))\n",
        "model_m.add(MaxPooling1D(10))\n",
        "model_m.add(Flatten())\n",
        "model_m.add(Dropout(0.50))\n",
        "model_m.add(Dense(64, kernel_regularizer = l2(1e-5), activation = 'relu', kernel_constraint = max_norm(1.)))\n",
        "model_m.add(Dense(6, kernel_regularizer = l2(1e-5),activation = 'sigmoid',kernel_constraint = max_norm(1.)))\n",
        "model_m.summary()\n",
        "model_m.compile(loss = \"binary_crossentropy\", optimizer = adam(lr = 0.001), metrics = [\"acc\"])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 100, 128)          16512     \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 100, 128)          65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 100, 128)          512       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 100, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 50, 128)           163968    \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 50, 128)           163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 50, 128)           512       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                41024     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 452,550\n",
            "Trainable params: 452,038\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bn_KeGQcgVp3",
        "colab_type": "code",
        "outputId": "db928975-9776-4ead-d6f9-ce8e10b4bc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        }
      },
      "cell_type": "code",
      "source": [
        "i = Input((100,32))\n",
        "o = TCN(nb_filters = 64, kernel_size = 8, nb_stacks = 1, dropout_rate = 0.4, dilations = [2,4,6,8])(i) \n",
        "#o = Dense(64, activation = 'elu', kernel_initializer = 'he_normal')(o)\n",
        "o = Dense(6, activation = 'sigmoid', kernel_initializer = 'he_normal')(o)\n",
        "model1 = Model(inputs = [i], outputs = [o])\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 100, 32)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 100, 64)      2112        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 100, 64)      32832       conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 100, 64)      0           conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_9 (SpatialDro (None, 100, 64)      0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 100, 64)      32832       spatial_dropout1d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 100, 64)      0           conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_10 (SpatialDr (None, 100, 64)      0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 100, 64)      4160        conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 100, 64)      0           conv1d_17[0][0]                  \n",
            "                                                                 spatial_dropout1d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 100, 64)      32832       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 100, 64)      0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_11 (SpatialDr (None, 100, 64)      0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 100, 64)      32832       spatial_dropout1d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 100, 64)      0           conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_12 (SpatialDr (None, 100, 64)      0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 100, 64)      4160        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 100, 64)      0           conv1d_20[0][0]                  \n",
            "                                                                 spatial_dropout1d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 100, 64)      32832       add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 100, 64)      0           conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_13 (SpatialDr (None, 100, 64)      0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 100, 64)      32832       spatial_dropout1d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 100, 64)      0           conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_14 (SpatialDr (None, 100, 64)      0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 100, 64)      4160        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 100, 64)      0           conv1d_23[0][0]                  \n",
            "                                                                 spatial_dropout1d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 100, 64)      32832       add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 100, 64)      0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_15 (SpatialDr (None, 100, 64)      0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 100, 64)      32832       spatial_dropout1d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 100, 64)      0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_16 (SpatialDr (None, 100, 64)      0           activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 100, 64)      0           spatial_dropout1d_10[0][0]       \n",
            "                                                                 spatial_dropout1d_12[0][0]       \n",
            "                                                                 spatial_dropout1d_14[0][0]       \n",
            "                                                                 spatial_dropout1d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 64)           0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 6)            390         lambda_4[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 277,638\n",
            "Trainable params: 277,638\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q55giuyNCkWv",
        "colab_type": "code",
        "outputId": "71c027ef-2888-4523-e4ff-357663697434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "cell_type": "code",
      "source": [
        "F1 = 16\n",
        "from keras.layers import SpatialDropout2D\n",
        "D = 4 ; F2 = D * F1 \n",
        "nb_classes = 6; norm_rate = 1.\n",
        "kernLength = 10\n",
        "Chans = 32\n",
        "Samples = 100 \n",
        "\n",
        "input1  = Input((100,32))\n",
        "block1 = Lambda(lambda x: K.expand_dims(x), output_shape = (100,32,1))(input1)\n",
        "#block1 = ExpandDimension(axis = -1)(input1)\n",
        "block1 = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                                   input_shape = (1, Chans, Samples),\n",
        "                                   use_bias = False)(block1)\n",
        "block1 = BatchNormalization(axis = -1)(block1)\n",
        "block1  = DepthwiseConv2D((1,Chans), use_bias = False, \n",
        "                               depth_multiplier = 4,\n",
        "                               depthwise_constraint = max_norm(1.))(block1)\n",
        "\n",
        "block1 = BatchNormalization(axis = -1)(block1)\n",
        "block1  = Activation('elu')(block1)\n",
        "block1 =  AveragePooling2D((4,1))(block1)\n",
        "block1 = SpatialDropout2D(0.50)(block1)\n",
        "\n",
        "block2 =  SeparableConv2D(F2, (1, 16), use_bias = False, padding = 'same')(block1)\n",
        "block2 = BatchNormalization(axis = -1)(block2)\n",
        "block2 = Activation('elu')(block2)\n",
        "block2 = AveragePooling2D((5,1))(block2)\n",
        "blok2 = SpatialDropout2D(0.50)(block2)\n",
        "\n",
        "block2 = Flatten()(block2)\n",
        "out = Dense(6, activation = 'sigmoid')(block2)\n",
        "\n",
        "\n",
        "##################################################################\n",
        "model2 = Model(inputs = [input1], outputs = [out])\n",
        "model2.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 100, 32, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 100, 32, 16)       160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 100, 32, 16)       64        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_2 (Depthwis (None, 100, 1, 64)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100, 1, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 25, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 25, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 25, 1, 64)         5120      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 25, 1, 64)         256       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 25, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 5, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 1926      \n",
            "=================================================================\n",
            "Total params: 9,830\n",
            "Trainable params: 9,542\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nU3xPyuTuD-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#freeze \n",
        "for layer in model1.layers[-7:]:\n",
        "  layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TGgiuVCg6r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_m.compile(loss='binary_crossentropy', \n",
        "              optimizer = adam(clipnorm = .10), \n",
        "              metrics = ['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NNA3Hw3cf2Pw",
        "colab_type": "code",
        "outputId": "f70edad0-4b72-46f8-9d24-83781013fa09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1151
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(33)\n",
        "model_m.fit_generator(train_gen,\n",
        "          validation_data = val_gen,\n",
        "          #batch_size = bs,\n",
        "          #shuffle = True, \n",
        "          #class_weight = weights, \n",
        "          epochs = 60, verbose=1, callbacks = [early, reduce_lr],\n",
        "          #steps_per_epoch = trainX.shape[0] // bs,  \n",
        "          #validation_steps = valX.shape[0]//bs\n",
        "         )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "21/21 [==============================] - 12s 576ms/step - loss: 0.1464 - acc: 0.9570 - val_loss: 0.0936 - val_acc: 0.9706\n",
            "Epoch 2/60\n",
            "21/21 [==============================] - 4s 191ms/step - loss: 0.0752 - acc: 0.9738 - val_loss: 0.0869 - val_acc: 0.9707\n",
            "Epoch 3/60\n",
            "21/21 [==============================] - 4s 195ms/step - loss: 0.0638 - acc: 0.9753 - val_loss: 0.0861 - val_acc: 0.9714\n",
            "Epoch 4/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0599 - acc: 0.9766 - val_loss: 0.0838 - val_acc: 0.9728\n",
            "Epoch 5/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0570 - acc: 0.9773 - val_loss: 0.0796 - val_acc: 0.9742\n",
            "Epoch 6/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0546 - acc: 0.9782 - val_loss: 0.0815 - val_acc: 0.9724\n",
            "Epoch 7/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0539 - acc: 0.9792 - val_loss: 0.0815 - val_acc: 0.9741\n",
            "Epoch 8/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0521 - acc: 0.9795 - val_loss: 0.0751 - val_acc: 0.9741\n",
            "Epoch 9/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0508 - acc: 0.9796 - val_loss: 0.0802 - val_acc: 0.9736\n",
            "Epoch 10/60\n",
            "21/21 [==============================] - 4s 195ms/step - loss: 0.0510 - acc: 0.9800 - val_loss: 0.0841 - val_acc: 0.9732\n",
            "Epoch 11/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0498 - acc: 0.9801 - val_loss: 0.0768 - val_acc: 0.9736\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "Epoch 12/60\n",
            "21/21 [==============================] - 4s 198ms/step - loss: 0.0470 - acc: 0.9808 - val_loss: 0.0776 - val_acc: 0.9755\n",
            "Epoch 13/60\n",
            "21/21 [==============================] - 4s 192ms/step - loss: 0.0488 - acc: 0.9807 - val_loss: 0.0793 - val_acc: 0.9734\n",
            "Epoch 14/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0469 - acc: 0.9813 - val_loss: 0.0820 - val_acc: 0.9744\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
            "Epoch 15/60\n",
            "21/21 [==============================] - 4s 195ms/step - loss: 0.0475 - acc: 0.9807 - val_loss: 0.0806 - val_acc: 0.9741\n",
            "Epoch 16/60\n",
            "21/21 [==============================] - 4s 195ms/step - loss: 0.0477 - acc: 0.9808 - val_loss: 0.0810 - val_acc: 0.9735\n",
            "Epoch 17/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0470 - acc: 0.9813 - val_loss: 0.0756 - val_acc: 0.9758\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
            "Epoch 18/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0460 - acc: 0.9817 - val_loss: 0.0796 - val_acc: 0.9743\n",
            "Epoch 19/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0453 - acc: 0.9814 - val_loss: 0.0762 - val_acc: 0.9738\n",
            "Epoch 20/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0451 - acc: 0.9815 - val_loss: 0.0775 - val_acc: 0.9744\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
            "Epoch 21/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0460 - acc: 0.9811 - val_loss: 0.0770 - val_acc: 0.9742\n",
            "Epoch 22/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0453 - acc: 0.9820 - val_loss: 0.0785 - val_acc: 0.9745\n",
            "Epoch 23/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0433 - acc: 0.9824 - val_loss: 0.0744 - val_acc: 0.9753\n",
            "Epoch 24/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0444 - acc: 0.9818 - val_loss: 0.0770 - val_acc: 0.9749\n",
            "Epoch 25/60\n",
            "21/21 [==============================] - 4s 194ms/step - loss: 0.0427 - acc: 0.9828 - val_loss: 0.0758 - val_acc: 0.9755\n",
            "Epoch 26/60\n",
            "21/21 [==============================] - 4s 193ms/step - loss: 0.0427 - acc: 0.9826 - val_loss: 0.0840 - val_acc: 0.9735\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb91494e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "metadata": {
        "id": "cza6m3PMevR2",
        "colab_type": "code",
        "outputId": "a4a03cec-ff0b-42c6-9284-d1277fa6651d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "cell_type": "code",
      "source": [
        "print(subj[0])\n",
        "model_m.save('/content/conv-lstm%d' % subj[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3ffffb9d8412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/conv-lstm%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_m' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3jRyNM5uY3yR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-FgD_LYSHMU",
        "colab_type": "code",
        "outputId": "035fe2ac-9ad1-4d80-cdec-bce5b6ed5171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "cell_type": "code",
      "source": [
        "#Densnet \n",
        "#!git clone https://github.com/ankitvgupta/densenet_1d.git\n",
        "#os.chdir(\"densenet_1d\")\n",
        "#!python setup.py install \n",
        "#os.chdir(\"/content\")\n",
        "\n",
        "i = Input(shape=(100,32))\n",
        "out = Bidirectional(LSTM(8,return_sequences = False, dropout = 0.2))(i)\n",
        "#out = GlobalAveragePooling1D()(out)\n",
        "out = Dense(6, kernel_initializer = 'he_uniform')(out)\n",
        "model = Model(inputs=[i],outputs=[out])\n",
        "model.summary()\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = adam(lr = 0.001), metrics = [\"acc\"])\n",
        " \n",
        "\n",
        "model.fit_generator(train_gen,\n",
        "          validation_data = val_gen,\n",
        "          #batch_size = bs,\n",
        "          #shuffle = True, \n",
        "          #class_weight = weights, \n",
        "          epochs = 5, verbose=1, callbacks = [reduce_lr],\n",
        "          #steps_per_epoch = trainX.shape[0] // bs,  \n",
        "          #validation_steps = valX.shape[0]//bs\n",
        "         )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_21 (InputLayer)        (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 16)                2624      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 2,726\n",
            "Trainable params: 2,726\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "25/25 [==============================] - 27s 1s/step - loss: 0.7932 - acc: 0.6679 - val_loss: 0.5080 - val_acc: 0.7282\n",
            "Epoch 2/5\n",
            "25/25 [==============================] - 14s 555ms/step - loss: 0.5435 - acc: 0.7154 - val_loss: 0.4222 - val_acc: 0.7579\n",
            "Epoch 3/5\n",
            "25/25 [==============================] - 14s 556ms/step - loss: 0.4730 - acc: 0.7319 - val_loss: 0.3939 - val_acc: 0.7704\n",
            "Epoch 4/5\n",
            "25/25 [==============================] - 14s 557ms/step - loss: 0.4249 - acc: 0.7404 - val_loss: 0.3927 - val_acc: 0.7670\n",
            "Epoch 5/5\n",
            "25/25 [==============================] - 14s 560ms/step - loss: 0.4275 - acc: 0.7414 - val_loss: 0.3602 - val_acc: 0.7694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6038bbd780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "aSDvGT7SuH0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyVZ6dAn9vAT",
        "colab_type": "code",
        "outputId": "b20d48b9-c666-40b7-ee45-1a291ec37766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "#model.save('/content/EEGnet%s.hdf5' % subj[0])\n",
        "#!gsutil -m cp /content/greatmodel5.hdf5 gs://peijinbucket\n",
        "model = load_model('/content/EEGnet1.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kbZATupbg-gR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jOOP3SNmmfv5",
        "colab_type": "code",
        "outputId": "86c6f519-20fc-4437-8782-0c62dd696c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "X_test = np.transpose(X_test, (1,0))\n",
        "X_test_filtered = mne.filter.filter_data(data = X_test, \n",
        "                                         sfreq=500, l_freq=0.1, \n",
        "                                         h_freq=40, picks = None)\n",
        "X_test_filtered = np.transpose(X_test_filtered, (1,0))\n",
        "#X_test_filtered = X_test_filtered ** 2 \n",
        "X_test_filtered = somescaler.transform(X_test_filtered)\n",
        "testdf = np.concatenate([X_train[-1000:],X_test_filtered])\n",
        "testdf.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up band-pass filter from 0.1 - 40 Hz\n",
            "l_trans_bandwidth chosen to be 0.1 Hz\n",
            "h_trans_bandwidth chosen to be 10.0 Hz\n",
            "Filter length of 16501 samples (33.002 sec) selected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(253400, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "metadata": {
        "id": "g1p7IfZQiMs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_gen = TimeseriesGenerator(testdf, targets = np.zeros(len(testdf)),length = 1000, sampling_rate = 10, \n",
        "                                  shuffle = False, \n",
        "                                  stride=1, batch_size=1000)\n",
        "#len(test_gen)\n",
        "#test_gen[0][0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nx8u0hnCSW6J",
        "colab_type": "code",
        "outputId": "dbb6f0d6-b65d-4378-d454-28b3c30f2360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_preds = model1.predict_generator(test_gen, verbose = 1)\n",
        "test_preds.shape\n",
        "print(subj)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "253/253 [==============================] - 25s 99ms/step\n",
            "[9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7mfEm3Nnqjm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# submission file\n",
        "os.chdir('/content')\n",
        "subject = subj[0]\n",
        "cols = ['HandStart','FirstDigitTouch',\n",
        "        'BothStartLoadPhase','LiftOff',\n",
        "        'Replace','BothReleased']\n",
        "submission_file = 'tcn_take2_0%d.csv' % subject\n",
        "# create pandas object for submission\n",
        "submission = pd.DataFrame(index = ids,\n",
        "                          columns = cols,\n",
        "                          data = test_preds)\n",
        "\n",
        "# write file\n",
        "submission.to_csv(submission_file,index_label='id',float_format='%.3f')#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uQWtrkOKryV",
        "colab_type": "code",
        "outputId": "04d4defa-7acd-4ec4-b9aa-52346263a457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp /content/tcn_take2_09.csv gs://peijinbucket/lstm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/tcn_take2_09.csv [Content-Type=text/csv]...\n",
            "/ [1/1 files][ 13.6 MiB/ 13.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/13.6 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IlpPZzQGr1tL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import pi\n",
        "from math import cos\n",
        "from math import floor\n",
        "from keras import backend\n",
        "class CosineAnnealingLearningRateSchedule(Callback):\n",
        "\t# constructor\n",
        "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
        "\t\tself.epochs = n_epochs\n",
        "\t\tself.cycles = n_cycles\n",
        "\t\tself.lr_max = lrate_max\n",
        "\t\tself.lrates = list()\n",
        "\n",
        "\t# calculate learning rate for an epoch\n",
        "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
        "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
        "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
        "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
        "\n",
        "\t# calculate and set learning rate at the start of the epoch\n",
        "\tdef on_epoch_begin(self, epoch, logs=None):\n",
        "\t\t# calculate learning rate\n",
        "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
        "\t\t# set learning rate\n",
        "\t\tbackend.set_value(self.model.optimizer.lr, lr)\n",
        "\t\t# log value\n",
        "\t\tself.lrates.append(lr)\n",
        "n_epochs = 60\n",
        "n_cycles = n_epochs // 10\n",
        "ca = CosineAnnealingLearningRateSchedule(n_epochs, n_cycles, lrate_max = 0.0011)\n",
        "#checkpointer = ModelCheckpoint(filepath = \"weights1_6.hdf5\", verbose = 1, save_best_only = True, monitor = \"val_loss\")\n",
        "#early = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 20, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJ18bKdMMjZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp -r gs://peijinbucket/lstm/tcn_take2/ /content "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9o--nzzObQy",
        "colab_type": "code",
        "outputId": "bf43e51c-7fa5-476c-9970-eff31561b456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'tcn_take2_12.csv',\n",
              " 'test',\n",
              " 'tcn_take2_07.csv',\n",
              " 'tcn_take2_02.csv',\n",
              " 'tcn_take2_11.csv',\n",
              " 'eeg_train',\n",
              " 'tcn_take2_03.csv',\n",
              " 'eegnet_02.csv',\n",
              " 'eegnet_final.csv',\n",
              " 'eegnet_03.csv',\n",
              " 'tcn_take2_09.csv',\n",
              " 'eegnet_01.csv',\n",
              " 'adc.json',\n",
              " 'tcn_take2_06.csv',\n",
              " 'LSTM_final.csv',\n",
              " 'tcn_take2_05.csv',\n",
              " 'tcn_take2_04.csv',\n",
              " 'eegnet_04.csv',\n",
              " 'lstm',\n",
              " 'OpenNE',\n",
              " 'tcn_take2_01.csv',\n",
              " 'tcn_take2_10.csv',\n",
              " 'tcn_take2_08.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "metadata": {
        "id": "m7Np7k76Mca5",
        "colab_type": "code",
        "outputId": "0dbb3d5d-44ef-4df3-b43b-e1f291014346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "raw = [] \n",
        "fnames =  glob('/content/tcn_take2_*.csv')\n",
        "\n",
        "print(len(fnames))\n",
        "fnames = np.sort(fnames)\n",
        "print(fnames)\n",
        "\n",
        "for fname in fnames:\n",
        "    dat = pd.read_csv(fname)\n",
        "    raw.append(dat)\n",
        "final_submission = pd.concat(raw)\n",
        "final_submission.shape\n",
        "final_submission.to_csv(\"tcn_take2_final.csv\", sep = \",\", header = True, index = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "['/content/tcn_take2_01.csv' '/content/tcn_take2_02.csv'\n",
            " '/content/tcn_take2_03.csv' '/content/tcn_take2_04.csv'\n",
            " '/content/tcn_take2_05.csv' '/content/tcn_take2_06.csv'\n",
            " '/content/tcn_take2_07.csv' '/content/tcn_take2_08.csv'\n",
            " '/content/tcn_take2_09.csv' '/content/tcn_take2_10.csv'\n",
            " '/content/tcn_take2_11.csv' '/content/tcn_take2_12.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lbxj2EsWcB0w",
        "colab_type": "code",
        "outputId": "37ff021d-fd4a-4d04-f4e3-266b109eaeab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "final_submission.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3144171, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "metadata": {
        "id": "hOAAiJ1lHdgv",
        "colab_type": "code",
        "outputId": "b1e83da4-dff0-4456-e017-dbf2a8b8a841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp /content/tcn_take2_final.csv gs://peijinbucket/lstm/tcn_take2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/tcn_take2_final.csv [Content-Type=text/csv]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "/ [1/1 files][175.5 MiB/175.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/175.5 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c1zjPM8tOih9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('/content/tcn_take2_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}